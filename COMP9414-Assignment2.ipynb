{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryan5859/Python-Stock-Project-Coursera/blob/main/COMP9414-Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8Em327q_1U6"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/UNSW-COMP9414/Assignment2/blob/main/COMP9414-Assignment2.ipynb)\n",
        "\n",
        "# COMP9414 24T3 - Assignment 2 - Neural Networks, Decision Trees and Random Forests\n",
        "\n",
        "## UNSW Sydney\n",
        "\n",
        "Designed by Gustavo Batista.\n",
        "\n",
        "Last change: 20th October, 2024."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW-Mfui4_1U8"
      },
      "source": [
        "Aryan Tiwari - z5547650"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZzjIq5z_1U9"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "**Submission deadline:** Friday, 8th November 2024, at 17:00:00 AEDT.\n",
        "\n",
        "**Submission:** You can submit your solution via the give system using the command ``give cs9414 ass2 ass2.ipynb``.\n",
        "\n",
        "**Instructions:**\n",
        "* This is an **individual** assignment.\n",
        "* Write your name and zID on the top of this Jupyter Notebook.\n",
        "* You can only use the libraries listed in this notebook\n",
        "* You can reuse any piece of source code developed in the tutorials.\n",
        "* Do not modify the existing code in this notebook except to answer the questions. The cells that should be modified are indicated.\n",
        "* If you want to submit additional code (e.g., for generating plots), write it at the end of the notebook.\n",
        "* This notebook is worth **75** marks and will be rescaled to **25** marks.\n",
        "\n",
        "**Late Submission Policy:** A 5% reduction of the assignment value (i.e. 1.25 marks) will be applied per day for late submissions. For example, if an assignment gets an on-time mark of $20/25$ but is submitted three days late, the penalty will be $3*1.25 = 3.75$, so the final mark will be $16.25$. After five days, the assignment total mark will be reduced to 0 ($100\\%$ reduction). An assignment is considered one day late if submitted any time after the submission deadline, up to 24 hours past it.\n",
        "\n",
        "**Plagiarism:**\n",
        "\n",
        "Remember that ALL work submitted for this assignment must be your own work, and no sharing or copying of code or answers is allowed. You may discuss the assignment with other students but must not collaborate to develop answers to the questions. You may use code from the Internet only with suitable attribution of the source. You may not use ChatGPT or any similar software to generate any part of your explanations, evaluations or code. Do not use public code repositories on sites such as GitHub or file-sharing sites such as Google Drive to save any part of your work &ndash; make sure your code repository or cloud storage is private, and do not share any links. This also applies after you have finished the course, as we do not want next year’s students accessing your solution, and plagiarism penalties can still apply after the course has finished.\n",
        "\n",
        "All submitted assignments will be run through plagiarism detection software to detect similarities to other submissions, including from past years. You should **carefully** read the UNSW policy on academic integrity and plagiarism (linked from the course web page), noting, in particular, that collusion (working together on an assignment or sharing parts of assignment solutions) is a form of plagiarism.\n",
        "\n",
        "Finally, do not use any contract cheating “academies” or online “tutoring” services. This counts as serious misconduct with heavy penalties up to automatic failure of the course with 0 marks and expulsion from the university for repeat offenders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAuY8Y1b_1U_"
      },
      "source": [
        "## Technical prerequisites\n",
        "\n",
        "These are the libraries you are allowed to use. No other libraries will be accepted. Make sure you are using Python 3."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List of all necessary libraries\n",
        "libraries = [\n",
        "    # General Purpose\n",
        "    \"numpy\", \"pandas\", \"scipy\", \"matplotlib\", \"seaborn\", \"requests\",\n",
        "    \"beautifulsoup4\", \"pytest\", \"pillow\",\n",
        "\n",
        "    # Machine Learning\n",
        "    \"scikit-learn\", \"xgboost\", \"lightgbm\", \"catboost\", \"statsmodels\",\n",
        "\n",
        "    # Deep Learning and AI\n",
        "    \"tensorflow\", \"keras\", \"torch\", \"transformers\", \"opencv-python\",\n",
        "    \"nltk\", \"spacy\", \"gym\", \"tqdm\",\n",
        "\n",
        "    # Data Science and Visualization\n",
        "    \"plotly\", \"dash\", \"bokeh\", \"geopandas\", \"networkx\",\n",
        "\n",
        "    # Utilities\n",
        "    \"joblib\", \"dill\", \"dataclasses\", \"pydantic\",\"tabulate\",\"keras-tuner\",\"keras_tuner\"\n",
        "]\n",
        "\n",
        "# Install all libraries\n",
        "for lib in libraries:\n",
        "    os.system(f\"pip install {lib}\")\n",
        "\n",
        "print(\"All libraries have been installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds10JRoB_9Pj",
        "outputId": "05075284-4af6-4368-ab9a-cae79cb730b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries have been installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A48JHg_7_1VB"
      },
      "outputs": [],
      "source": [
        "# These are the allowed libraries. You can add other libraries used in the tutorials.\n",
        "\n",
        "# Common Python libraries\n",
        "import math\n",
        "import copy\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import matplotlib as mp\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# Scikit-Learn libraries for data preprocessing and model assessment\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Libraries for the tree models\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "\n",
        "# Scikit-learn libraries for hyperparameter tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Tensorflow/keras libraries for shallow and deep-learning models\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "\n",
        "# Keras Tuner libraries for hyperparameter tuning\n",
        "from keras_tuner import HyperModel\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "\n",
        "# Libraries to present results in tabular format\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ycN2xyQ_1VC"
      },
      "source": [
        "This assignment compares three Machine Learning approaches: Neural Networks, Decision Trees, and Random Forests. We will assess these approaches in five benchmark datasets with diverse characteristics.\n",
        "\n",
        "We would like to test a few hypotheses based on common Machine Learning wisdom and misconceptions.\n",
        "\n",
        "1. Neural networks are the best general classifiers regarding prediction quality (accuracy, error rate, precision, recall, etc.).\n",
        "2. Neural networks are time-consuming for training as fitting model parameters is slow and has many hyperparameters.\n",
        "3. Random forests are an excellent compromise between classification performance and hyperparameter tuning. They can often provide competitive accuracy without requiring much hyperparameter tuning.\n",
        "4. Neural networks are data-hungry and perform poorly in small datasets.\n",
        "5. Decision trees offer model interpretability but are not competitive in accuracy.\n",
        "6. Neural networks are the best models when learning from unstructured data such as images.\n",
        "7. Random forests are the best models when learning from structured data such as a tabular dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55kJes3q_1VD"
      },
      "source": [
        "## Task 0 - Datasets description, downloading and loading the data into a Pandas dataframe\n",
        "\n",
        "We have selected five publicly available benchmark datasets:\n",
        "\n",
        "1. **UCI adult income dataset.** This is a binary classification dataset in which we want to predict if a person earns more than $50k/year. It is a mid-size dataset (48K examples) with 14 features of mixed data types (categorical and continuous) with missing values.\n",
        "\n",
        "2. **Forest cover type dataset.** This is a large multi-class dataset with 580K examples and 54 features of mixed types. The objective is to predict the type of forest cover based on features such as soil type, elevation, and slope.\n",
        "\n",
        "3. **California housing prices**. This is a regression problem in which we want to predict housing prices based on numerical features, such as population, median income and location. It has 20K instances and nine features.\n",
        "\n",
        "4. **Fashion MNIST dataset**. It is an image classification dataset that is very similar to MNIST. Images are $28 \\times 28$ grayscale pixels. The objective is to classify ten different types of clothing. It has 60k training and 10K test instances.\n",
        "\n",
        "5. **Credit card fraud detection**. This is a binary classification dataset for detecting fraudulent transactions in credit card data. It is highly imbalanced, meaning that most transactions are normal, with some rare fraud cases. It has 284K instances and 30 numerical features.\n",
        "\n",
        "This table summarises the datasets.\n",
        "\n",
        "| Dataset                          | Problem Type        | Feature Type                          | Size        | Notable Challenge                                    |\n",
        "|-----------------------------------|---------------------|---------------------------------------|-------------|------------------------------------------------------|\n",
        "| **UCI Adult Income**              | Binary Classification| Categorical and Numerical             | 48,000      | Mix of feature types with missing values           |\n",
        "| **Forest Cover Type**             | Multi-class Classification | Categorical and Numerical       | 580,000     | Large dataset with mix of feature types      |\n",
        "| **California Housing Prices**     | Regression          | Numerical                              | 20,000      | Regression task      |\n",
        "| **Fashion MNIST**                 | Multi-class Classification (Image)| Image (grayscale)        | 60,000      | Weak features in the form of individual pixels brightness       |\n",
        "| **Credit Card Fraud Detection**   | Binary Classification (Imbalanced)| Numerical                | 284,000     | Highly imbalanced dataset        |\n",
        "\n",
        "Let's start by downloading the data from GitHub. The cell below will download and save the data into a local ``data`` folder. We will use the data later to train and assess our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klEb-SME_1VE",
        "outputId": "10481b08-1cad-4b66-b45d-880948695a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading adult.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/adult/adult.zip...\n",
            "Downloading covertype.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/covertype/covertype.zip...\n",
            "Downloading california_housing.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/california_housing/california_housing.zip...\n",
            "Downloading creditcard.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/creditcard/creditcard.zip...\n",
            "Downloading fashion_mnist.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/fashion_mnist/fashion_mnist.zip...\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It downloads and unzips the datasets to your local disk.\n",
        "\n",
        "def download_and_extract(url, extract_to):\n",
        "    \"\"\"\n",
        "    Download a zip file from the URL and extract it to the specified directory.\n",
        "\n",
        "    Parameters:\n",
        "    - url (str): The URL of the zip file to download.\n",
        "    - extract_to (str): The directory where the zip file's contents will be extracted.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Get the file, dataset names from the URL\n",
        "    zip_filename = url.split(\"/\")[-1]\n",
        "    dataset_name = zip_filename.split(\".\")[0]\n",
        "\n",
        "    # Each dataset will have its folder\n",
        "    extract_to = extract_to + \"/\" + dataset_name\n",
        "\n",
        "    # Download the zip file\n",
        "    print(f\"Downloading {zip_filename} from {url}...\")\n",
        "    response = requests.get(url)\n",
        "    with open(zip_filename, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "    # Create the extraction directory if it doesn't exist\n",
        "    if not os.path.exists(extract_to):\n",
        "        os.makedirs(extract_to)\n",
        "\n",
        "    # Unzip the file\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "    # Remove the zip files\n",
        "    os.remove(zip_filename)\n",
        "\n",
        "# These are the URLs to the datasets. We have hosted the data on GitHub.\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/adult/adult.zip\",\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/covertype/covertype.zip\",\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/california_housing/california_housing.zip\",\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/creditcard/creditcard.zip\",\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/fashion_mnist/fashion_mnist.zip\",\n",
        "]\n",
        "\n",
        "for i, url in enumerate(urls):\n",
        "    download_and_extract(url, \"data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdovQ_MR_1VG"
      },
      "source": [
        "### Loading data into pandas\n",
        "\n",
        "The datasets are well-diversified in size (number of examples and features), number of class labels, feature types (continuous and discrete), class distribution, and presence of missing data.\n",
        "\n",
        "All datasets have pre-defined training and testing splits. We will use the training set to train the models and choose hyperparameters. You may further split the training set into training and validation sets. The test set should only be used to assess and compare the models.\n",
        "\n",
        "The next cell has a supporting function that loads a specified dataset training and test sets into a pandas' dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1ZuE8km_1VH",
        "outputId": "984c4be2-3145-40b6-d489-746f28f0abc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data/adult...\n",
            "Train Features Shape: (32561, 14), Train Labels Shape: (32561, 1)\n",
            "Test Features Shape: (16281, 14), Test Labels Shape: (16281, 1)\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It is a helper function that loads data from this into a Pandas dataframe.\n",
        "\n",
        "def load_train_test_data(path):\n",
        "    \"\"\"\n",
        "    Loads the train and test CSV files and returns them split into features (X) and labels (y).\n",
        "\n",
        "    Parameters:\n",
        "    - path (str): Path to the train and test CSV files.\n",
        "\n",
        "    Returns:\n",
        "    - X_train (DataFrame): Features of the training dataset.\n",
        "    - y_train (DataFrame): Labels of the training dataset.\n",
        "    - X_test (DataFrame): Features of the test dataset.\n",
        "    - y_test (DataFrame): Labels of the test dataset.\n",
        "    \"\"\"\n",
        "    # Load the training and testing data\n",
        "    train_df = pd.read_csv(f\"{path}/train.csv\")\n",
        "    test_df = pd.read_csv(f\"{path}/test.csv\")\n",
        "\n",
        "    # Select class label columns (those starting with 'Target')\n",
        "    y_train = train_df.filter(regex='^Target')\n",
        "    y_test = test_df.filter(regex='^Target')\n",
        "\n",
        "    # Select feature columns (all columns except the ones with 'Target' prefix)\n",
        "    X_train = train_df.drop(columns=y_train.columns)\n",
        "    X_test = test_df.drop(columns=y_test.columns)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Example usage:\n",
        "path = \"data/adult\"\n",
        "print(f\"Loading {path}...\")\n",
        "X_train, y_train, X_test, y_test = load_train_test_data(path)\n",
        "\n",
        "print(f\"Train Features Shape: {X_train.shape}, Train Labels Shape: {y_train.shape}\")\n",
        "print(f\"Test Features Shape: {X_test.shape}, Test Labels Shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp_9v2i2_1VI"
      },
      "source": [
        "## Task 1 [14 Marks] - Data preprocessing\n",
        "\n",
        "Your first task is to preprocess the datasets. Preprocessing usually involves data cleaning and transformation to improve data quality and prepare the data for the specific requirements of the learning approaches.\n",
        "\n",
        "For data preparation, we have the following tasks:\n",
        "1. **Missing imputation (all models)**: The adult dataset has missing values, and none of our learning algorithm implementations can directly handle missing data. Two missing data treatments are eliminating the rows with missing data or replacing the missing values with estimated ones. *Mean imputation*, as the name suggests, replaces missing values with the attribute mean, median (continuous features) or mode (discrete features). These statistics must only be estimated in the training set.\n",
        "2. **Feature encoding (all models)**: Neural networks, tree and random forest implementations available in the Scikit-Learn library do not handle categorical attributes directly. Therefore, these attributes need to be converted into numerical attributes. Although several encoding approaches exist, we will use one-hot encoding, as it is simple and recommended for categorical features with a small cardinality.\n",
        "3. **Class attribute encoding (neural networks only)**: Neural networks also need a one-hot encoding for the class attribute. This step is not necessary for the tree models.\n",
        "4. **Rescaling attribute values (neural networks only)**: The neural network's training benefits from rescaling the attribute values. In this task, we will convert each attribute to a number in the 0 to 1 range by using a simple linear rescaling: $x_s = \\frac{x-min_f}{max_f-min_f}$, where $x_s$ is the recalled $x$ value, $min_f$ is the minimum and $max_f$ the maximum values for feature $f$ in the training data.\n",
        "\n",
        "Tree models typically do not use class encoding and rescaling. The reason is twofold: first, this preprocessing does not help these models fit better parameters; second, tree models are known for their interpretability, and these manipulations create models that are not easier and often harder to understand. Feature encoding is also unnecessary for many tree model implementations, including the well-known [XGBoost](https://xgboost.readthedocs.io/en/stable/) and [LightGBM](https://lightgbm.readthedocs.io/en/stable/). Unfortunately, the Scikit-Learn implementation of tree models does not support categorical attributes.\n",
        "\n",
        "**Warning**: Leaking information from the test set to the training set, even if such information is aggregated data such as means, maximums, and minimums, is considered a serious methodological error. For instance, mean imputation should use the mean only in the training set. Similarly, the maximum and minimum for attribute rescaling should be calculated in the training set. Consequently, we may see values outside the range of 0-1 in the rescaled test set. This mimics the situation in which we find extreme values after the model deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mderMHuE_1VI"
      },
      "source": [
        "### Task 1.1 [6 Marks] - Missing data removal or imputation\n",
        "\n",
        "Create a function ``missing_data(X_train, X_test)`` that imputes missing values in the dataframes `X_train` and `X_test`. When the function returns, both dataframes should have no missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NlbuPckb_1VJ"
      },
      "outputs": [],
      "source": [
        "def missing_data(X_train, X_test):\n",
        "    \"\"\"Fill missing values in train and test datasets.\"\"\"\n",
        "    # Fill numerical columns with median values\n",
        "    for col in X_train.select_dtypes(include=['float64', 'int64']).columns:\n",
        "        median_val = X_train[col].median()\n",
        "        X_train[col].fillna(median_val, inplace=True)\n",
        "        X_test[col].fillna(median_val, inplace=True)\n",
        "\n",
        "    # Fill categorical columns with mode values\n",
        "    for col in X_train.select_dtypes(include=['object']).columns:\n",
        "        mode_val = X_train[col].mode()[0]\n",
        "        X_train[col].fillna(mode_val, inplace=True)\n",
        "        X_test[col].fillna(mode_val, inplace=True)\n",
        "\n",
        "    return X_train, X_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW_iYn43_1VJ"
      },
      "source": [
        "### Task 1.2 [4 Marks] - Feature and class encoding\n",
        "\n",
        "Let's implement a function ``encoding(X_train, X_test)`` that creates one-hot encodings for all categorical attributes. All categorical attributes are encoded as one-hot numeric features when the function returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wYzQxQRA_1VK"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def encoding(X_train, X_test):\n",
        "    \"\"\"Encode categorical variables using one-hot encoding.\"\"\"\n",
        "    # Select categorical columns\n",
        "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Set up the encoder with `sparse_output=False`\n",
        "    encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
        "\n",
        "    # Fit on training data and transform both train and test sets\n",
        "    X_train_encoded = encoder.fit_transform(X_train[categorical_cols])\n",
        "    X_test_encoded = encoder.transform(X_test[categorical_cols])\n",
        "\n",
        "    # Convert to DataFrame with appropriate column names\n",
        "    X_train_encoded = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "    X_test_encoded = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "    # Reset index to concatenate with original data\n",
        "    X_train_encoded.reset_index(drop=True, inplace=True)\n",
        "    X_test_encoded.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Concatenate encoded columns with the original numerical columns\n",
        "    X_train = pd.concat([X_train.drop(categorical_cols, axis=1).reset_index(drop=True), X_train_encoded], axis=1)\n",
        "    X_test = pd.concat([X_test.drop(categorical_cols, axis=1).reset_index(drop=True), X_test_encoded], axis=1)\n",
        "\n",
        "    return X_train, X_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drhnDZbD_1VK"
      },
      "source": [
        "#### Task 1.3 [4 Marks] - Rescaling attributes\n",
        "\n",
        "To conclude the pre-processing task, let's create a function ``rescale(X_train, X_test)`` that rescales all continuous attributes so that each attribute is between 0 and 1. When the function returns, all numerical attributes should be rescaled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NvCrAeX3_1VL"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def rescale(X_train, X_test):\n",
        "    \"\"\"Standardize features by removing the mean and scaling to unit variance.\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform both train and test data\n",
        "    X_train_rescaled = scaler.fit_transform(X_train)\n",
        "    X_test_rescaled = scaler.transform(X_test)\n",
        "\n",
        "    # Convert the arrays back to DataFrames with original column names\n",
        "    X_train_rescaled = pd.DataFrame(X_train_rescaled, columns=X_train.columns)\n",
        "    X_test_rescaled = pd.DataFrame(X_test_rescaled, columns=X_test.columns)\n",
        "\n",
        "    return X_train_rescaled, X_test_rescaled\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPDE9qF3_1VL"
      },
      "source": [
        "### Preprocessing the datasets\n",
        "\n",
        "In the cell below, we will call your functions to preprocess the datasets. We will create two versions of each dataset: the first is suitable for the tree models and will have no missing values and encoded attributes. The second will have no missing values, encoded categorical and class features, and numeric features rescaled. We will save these datasets for use later. The datasets pre-processed for trees will be saved in a ``tree`` folder. The datasets for neural networks will be saved in a ``nn`` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z5Ok8nf_1VL",
        "outputId": "c2994a45-2d3b-4d8c-9378-46abd604a975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: adult\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-d7b9788157ad>:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train[col].fillna(median_val, inplace=True)\n",
            "<ipython-input-8-d7b9788157ad>:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test[col].fillna(median_val, inplace=True)\n",
            "<ipython-input-8-d7b9788157ad>:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train[col].fillna(mode_val, inplace=True)\n",
            "<ipython-input-8-d7b9788157ad>:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test[col].fillna(mode_val, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: covertype\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-d7b9788157ad>:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train[col].fillna(median_val, inplace=True)\n",
            "<ipython-input-8-d7b9788157ad>:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test[col].fillna(median_val, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: california_housing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-d7b9788157ad>:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train[col].fillna(median_val, inplace=True)\n",
            "<ipython-input-8-d7b9788157ad>:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test[col].fillna(median_val, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: fashion_mnist\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-d7b9788157ad>:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train[col].fillna(median_val, inplace=True)\n",
            "<ipython-input-8-d7b9788157ad>:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test[col].fillna(median_val, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: creditcard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-d7b9788157ad>:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train[col].fillna(median_val, inplace=True)\n",
            "<ipython-input-8-d7b9788157ad>:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test[col].fillna(median_val, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It calls your pre-processing functions and saves the preprocessed datasets on disk.\n",
        "\n",
        "datasets = [\"adult\", \"covertype\", \"california_housing\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(\"data/\" + dataset)\n",
        "\n",
        "    # Preprocessing for tree-based models\n",
        "    tree_path = f\"data/{dataset}/tree\"\n",
        "    if not os.path.exists(tree_path):\n",
        "        os.makedirs(tree_path)\n",
        "\n",
        "    # Handle missing data\n",
        "    X_train, X_test = missing_data(X_train, X_test)\n",
        "\n",
        "    # Apply encoding features\n",
        "    X_train_encoded, X_test_encoded = encoding(X_train, X_test)\n",
        "\n",
        "    # Concatenate X and y for train and test data.\n",
        "    # For decision trees, we do not encode the class attribute\n",
        "    train_tree = pd.concat([X_train_encoded, y_train.reset_index(drop=True)], axis=1)\n",
        "    test_tree = pd.concat([X_test_encoded, y_test.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # Save tree-preprocessed datasets\n",
        "    train_tree.to_csv(f\"{tree_path}/train.csv\", index=False)\n",
        "    test_tree.to_csv(f\"{tree_path}/test.csv\", index=False)\n",
        "\n",
        "    # Preprocessing for neural networks\n",
        "    nn_path = f\"data/{dataset}/nn\"\n",
        "    if not os.path.exists(nn_path):\n",
        "        os.makedirs(nn_path)\n",
        "\n",
        "    # Apply encoding class attribute. For a regression dataset, the next line should do nothing\n",
        "    y_train_encoded, y_test_encoded = encoding(y_train, y_test)\n",
        "\n",
        "    # Rescale the features\n",
        "    X_train_rescaled, X_test_rescaled = rescale(X_train_encoded, X_test_encoded)\n",
        "\n",
        "    # Concatenate X and y for train and test data\n",
        "    train_nn = pd.concat([X_train_rescaled, y_train_encoded.reset_index(drop=True)], axis=1)\n",
        "    test_nn = pd.concat([X_test_rescaled, y_test_encoded.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # Save nn-preprocessed datasets\n",
        "    train_nn.to_csv(f\"{nn_path}/train.csv\", index=False)\n",
        "    test_nn.to_csv(f\"{nn_path}/test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHB2j696_1VM"
      },
      "source": [
        "## Task 2 - [16 Marks] Model Training\n",
        "\n",
        "We have the data ready, and in this task, we will train some initial models for each dataset. We will refine the models later, but for now, we will create a swallow model for the neural network. The decision tree and the random forest models will use Scikit-Learn's default hyperparameter values for these models.\n",
        "\n",
        "The neural network will have three layers: the input layer ($i$), one hidden layer ($h$) and one output layer ($o$). We will use a simple rule-of-thumb for the number of units in the hidden layer: $D_h = \\sqrt{D_i * D_o}$. The other hyperparameters are similar to the ones used in the Week 07 tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KSdbkLM_1VM"
      },
      "source": [
        "### Task 2.1 [4 Marks] - Shallow neural net for classification\n",
        "\n",
        "Create a function ``train_shallow_net_class(X_train, y_train)`` that trains a shallow neural net for classification using the training data ``X_train`` and labels ``y_train``. Use the following hyperparameters:\n",
        "1. A single hidden layer with $D_h = \\text{round}(\\sqrt{D_i * D_o})$ units.\n",
        "2. ReLU activation in the hidden layer and softmax on the output layer.\n",
        "3. Categorical cross-entropy as loss function.\n",
        "4. Train for 30 epochs.\n",
        "5. Batch size of 32 instances.\n",
        "6. Validation split of 20% of the training data.\n",
        "7. Adam optimiser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YYSboPUy_1VN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "def train_shallow_net_class(X_train, y_train):\n",
        "    \"\"\"Creates and trains a basic neural network for a classification task.\"\"\"\n",
        "\n",
        "    # Get the number of input features (D_i) and output classes (D_o) from data\n",
        "    D_i = X_train.shape[1]  # Input layer size based on features\n",
        "    D_o = y_train.shape[1]  # Output layer size based on class labels\n",
        "\n",
        "    # Calculate hidden layer size (D_h) using a rule of thumb: sqrt(D_i * D_o)\n",
        "    D_h = round(np.sqrt(D_i * D_o))\n",
        "\n",
        "    # Build the neural network model\n",
        "    model = Sequential([\n",
        "        # Add hidden layer with ReLU activation\n",
        "        Dense(D_h, activation='relu', input_shape=(D_i,)),\n",
        "        # Add output layer with softmax activation for multi-class classification\n",
        "        Dense(D_o, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model with optimizer, loss function, and metrics for evaluation\n",
        "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model with the training data\n",
        "    # Set the number of epochs (repetitions) and batch size (samples per gradient update)\n",
        "    # Use 20% of data as validation to monitor performance during training\n",
        "    model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)\n",
        "\n",
        "    # Return the trained model\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OYHqpy6_1VN"
      },
      "source": [
        "The next cell will call your function to train a shallow model for each classification dataset, compute the training time, and test error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZS_QlVc_1VO",
        "outputId": "b7998946-706c-4b8b-bf2b-bd76740729ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: adult\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py:27: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(32, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m811/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2412 - loss: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py:27: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2412 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 2/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2410 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 3/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2374 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 4/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2403 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 5/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2399 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 6/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.2361 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 7/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.2411 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 8/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2397 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 9/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2385 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 10/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2395 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 11/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2390 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 12/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2380 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 13/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.2405 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 14/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.2428 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 15/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2447 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 16/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2368 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 17/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2367 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 18/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2359 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 19/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2400 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 20/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.2415 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 21/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2384 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 22/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2374 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 23/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2415 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 24/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2408 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 25/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2373 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 26/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2400 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 27/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2398 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 28/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2392 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 29/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2403 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 30/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2391 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.2366 - loss: 0.0000e+00\n",
            "Test error rate: 0.7638\n",
            "Runtime to train the model: 63.50580716133118 seconds\n",
            "Processing dataset: covertype\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.6336 - loss: 0.7823 - val_accuracy: 0.6645 - val_loss: 0.6774\n",
            "Epoch 2/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.6601 - loss: 0.6888 - val_accuracy: 0.6506 - val_loss: 0.7361\n",
            "Epoch 3/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.6480 - loss: 0.7500 - val_accuracy: 0.6404 - val_loss: 0.8216\n",
            "Epoch 4/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.6328 - loss: 0.8367 - val_accuracy: 0.6161 - val_loss: 0.8821\n",
            "Epoch 5/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.6191 - loss: 0.8848 - val_accuracy: 0.6105 - val_loss: 0.9385\n",
            "Epoch 6/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.5946 - loss: 0.9542 - val_accuracy: 0.5452 - val_loss: 1.0346\n",
            "Epoch 7/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.5755 - loss: 1.0109 - val_accuracy: 0.5325 - val_loss: 1.0876\n",
            "Epoch 8/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.5622 - loss: 1.0639 - val_accuracy: 0.5733 - val_loss: 1.0481\n",
            "Epoch 9/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - accuracy: 0.5527 - loss: 1.1003 - val_accuracy: 0.5691 - val_loss: 1.0678\n",
            "Epoch 10/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.5432 - loss: 1.1462 - val_accuracy: 0.5505 - val_loss: 1.1173\n",
            "Epoch 11/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2ms/step - accuracy: 0.5386 - loss: 1.1664 - val_accuracy: 0.5465 - val_loss: 1.1238\n",
            "Epoch 12/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.5335 - loss: 1.1776 - val_accuracy: 0.5527 - val_loss: 1.1242\n",
            "Epoch 13/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.5302 - loss: 1.1951 - val_accuracy: 0.5533 - val_loss: 1.1282\n",
            "Epoch 14/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2ms/step - accuracy: 0.5299 - loss: 1.2113 - val_accuracy: 0.5527 - val_loss: 1.2202\n",
            "Epoch 15/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.5247 - loss: 1.2525 - val_accuracy: 0.5549 - val_loss: 1.2152\n",
            "Epoch 16/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.5233 - loss: 1.2710 - val_accuracy: 0.5366 - val_loss: 1.1788\n",
            "Epoch 17/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.5233 - loss: 1.2825 - val_accuracy: 0.4477 - val_loss: 1.3806\n",
            "Epoch 18/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.5196 - loss: 1.3138 - val_accuracy: 0.5385 - val_loss: 1.2155\n",
            "Epoch 19/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.5193 - loss: 1.3363 - val_accuracy: 0.3680 - val_loss: 1.9007\n",
            "Epoch 20/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.5162 - loss: 1.3700 - val_accuracy: 0.5394 - val_loss: 1.3817\n",
            "Epoch 21/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - accuracy: 0.5159 - loss: 1.3707 - val_accuracy: 0.5382 - val_loss: 1.2930\n",
            "Epoch 22/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.5129 - loss: 1.4204 - val_accuracy: 0.5413 - val_loss: 1.4129\n",
            "Epoch 23/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2ms/step - accuracy: 0.5125 - loss: 1.4322 - val_accuracy: 0.5105 - val_loss: 1.2300\n",
            "Epoch 24/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.5106 - loss: 1.4492 - val_accuracy: 0.5327 - val_loss: 1.7265\n",
            "Epoch 25/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2ms/step - accuracy: 0.5081 - loss: 1.5094 - val_accuracy: 0.5109 - val_loss: 1.3268\n",
            "Epoch 26/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.5120 - loss: 1.5010 - val_accuracy: 0.4976 - val_loss: 1.4543\n",
            "Epoch 27/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.5079 - loss: 1.5133 - val_accuracy: 0.5131 - val_loss: 1.4487\n",
            "Epoch 28/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2ms/step - accuracy: 0.5089 - loss: 1.5508 - val_accuracy: 0.4244 - val_loss: 1.6535\n",
            "Epoch 29/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.5060 - loss: 1.5790 - val_accuracy: 0.4470 - val_loss: 1.6632\n",
            "Epoch 30/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.5063 - loss: 1.5563 - val_accuracy: 0.5090 - val_loss: 1.9838\n",
            "\u001b[1m6053/6053\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - accuracy: 0.5091 - loss: 1.9701\n",
            "Test error rate: 0.4902\n",
            "Runtime to train the model: 745.7078833580017 seconds\n",
            "Processing dataset: fashion_mnist\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7182 - loss: 0.5746 - val_accuracy: 0.7117 - val_loss: 0.9050\n",
            "Epoch 2/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7518 - loss: 0.7439 - val_accuracy: 0.7332 - val_loss: 1.1184\n",
            "Epoch 3/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7433 - loss: 1.1924 - val_accuracy: 0.7144 - val_loss: 1.1985\n",
            "Epoch 4/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7159 - loss: 1.8319 - val_accuracy: 0.5740 - val_loss: 2.4926\n",
            "Epoch 5/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.5726 - loss: 2.6177 - val_accuracy: 0.3536 - val_loss: 2.7723\n",
            "Epoch 6/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.3484 - loss: 3.2098 - val_accuracy: 0.3983 - val_loss: 3.0631\n",
            "Epoch 7/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.3352 - loss: 3.8183 - val_accuracy: 0.3612 - val_loss: 8.5828\n",
            "Epoch 8/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.3288 - loss: 4.2051 - val_accuracy: 0.3414 - val_loss: 2.5514\n",
            "Epoch 9/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.3327 - loss: 4.5926 - val_accuracy: 0.3343 - val_loss: 2.7380\n",
            "Epoch 10/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.3315 - loss: 5.0766 - val_accuracy: 0.3357 - val_loss: 3.5323\n",
            "Epoch 11/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.3323 - loss: 4.9010 - val_accuracy: 0.4116 - val_loss: 4.0274\n",
            "Epoch 12/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.3289 - loss: 5.5846 - val_accuracy: 0.3380 - val_loss: 2.8332\n",
            "Epoch 13/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.3249 - loss: 5.9262 - val_accuracy: 0.2768 - val_loss: 14.2528\n",
            "Epoch 14/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.3217 - loss: 6.9884 - val_accuracy: 0.3128 - val_loss: 4.3049\n",
            "Epoch 15/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 4ms/step - accuracy: 0.3194 - loss: 8.3118 - val_accuracy: 0.3122 - val_loss: 5.0197\n",
            "Epoch 16/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.3207 - loss: 6.5767 - val_accuracy: 0.2574 - val_loss: 14.7628\n",
            "Epoch 17/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.3157 - loss: 8.5994 - val_accuracy: 0.3830 - val_loss: 10.3200\n",
            "Epoch 18/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.3205 - loss: 8.2898 - val_accuracy: 0.3309 - val_loss: 3.6388\n",
            "Epoch 19/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.3116 - loss: 9.2478 - val_accuracy: 0.3153 - val_loss: 5.2169\n",
            "Epoch 20/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.3150 - loss: 9.3351 - val_accuracy: 0.3325 - val_loss: 3.6634\n",
            "Epoch 21/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.3161 - loss: 9.3042 - val_accuracy: 0.3039 - val_loss: 8.0753\n",
            "Epoch 22/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.3206 - loss: 8.9653 - val_accuracy: 0.3120 - val_loss: 6.7255\n",
            "Epoch 23/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.3204 - loss: 9.4316 - val_accuracy: 0.3243 - val_loss: 5.2881\n",
            "Epoch 24/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.3208 - loss: 9.6430 - val_accuracy: 0.3018 - val_loss: 7.4007\n",
            "Epoch 25/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.3095 - loss: 11.8327 - val_accuracy: 0.2856 - val_loss: 17.3336\n",
            "Epoch 26/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.3145 - loss: 11.5228 - val_accuracy: 0.3177 - val_loss: 4.6271\n",
            "Epoch 27/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.3151 - loss: 10.6754 - val_accuracy: 0.3165 - val_loss: 6.2158\n",
            "Epoch 28/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.3161 - loss: 11.9643 - val_accuracy: 0.3023 - val_loss: 9.9410\n",
            "Epoch 29/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.3115 - loss: 13.6417 - val_accuracy: 0.3302 - val_loss: 14.8477\n",
            "Epoch 30/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.3119 - loss: 12.8466 - val_accuracy: 0.2808 - val_loss: 16.1483\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.2795 - loss: 16.2220\n",
            "Test error rate: 0.7168\n",
            "Runtime to train the model: 196.7577724456787 seconds\n",
            "Processing dataset: creditcard\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py:27: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4734/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9984 - loss: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py:27: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 2/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 3/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 4/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 5/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 6/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 7/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 8/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 9/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 10/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 11/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 12/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 13/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 14/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 15/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 16/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 17/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 18/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 19/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.9984 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 20/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 21/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 22/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 23/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 24/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 25/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 26/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 27/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 28/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 29/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 30/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "\u001b[1m2967/2967\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9984 - loss: 0.0000e+00\n",
            "Test error rate: 0.0016\n",
            "Runtime to train the model: 307.1361835002899 seconds\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your shallow model using the classification datasets.\n",
        "\n",
        "results = defaultdict(dict)\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/nn/\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_shallow_net_class(X_train, y_train)\n",
        "    end = time.time()\n",
        "\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f'Test error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Neural Net\"].setdefault(dataset, {})\n",
        "    results[\"Neural Net\"][dataset][\"Prediction quality\"] =  1 - test_accuracy         # Error rate = 1 - accuracy\n",
        "    results[\"Neural Net\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUpQSWmN_1VO"
      },
      "source": [
        "### Task 2.2 [4 Marks] - Shallow neural net for regression\n",
        "\n",
        "Like the previous task, create a function ``train_shallow_net_regression(X_train, y_train)`` that trains a shallow neural net for regression using the training data ``X_train`` and labels ``y_train``. Use the following hyperparameters:\n",
        "1. A single hidden layer with $D_h = \\text{round}(\\sqrt{D_i * D_o})$ units.\n",
        "2. ReLU activation in the hidden layer and linear on the output layer.\n",
        "3. MSE loss function.\n",
        "4. Train for 30 epochs.\n",
        "5. Batch size of 32 instances.\n",
        "6. Validation split of 20% of the training data.\n",
        "7. Adam optimiser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mAgFJ9OC_1VP"
      },
      "outputs": [],
      "source": [
        "def train_shallow_net_regression(X_train, y_train):\n",
        "    \"\"\"Creates and trains a basic neural network for a regression task.\"\"\"\n",
        "\n",
        "    # Determine input layer size (D_i) from the number of features in X_train\n",
        "    D_i = X_train.shape[1]\n",
        "    # Output layer size (D_o) is 1, since regression usually has a single target\n",
        "    D_o = 1\n",
        "    # Calculate hidden layer size (D_h) using square root of (D_i * D_o)\n",
        "    D_h = round(np.sqrt(D_i * D_o))\n",
        "\n",
        "    # Define the neural network architecture\n",
        "    model = Sequential([\n",
        "        # Add hidden layer with ReLU activation\n",
        "        Dense(D_h, activation='relu', input_shape=(D_i,)),\n",
        "        # Add output layer with linear activation, suited for regression\n",
        "        Dense(D_o, activation='linear')\n",
        "    ])\n",
        "\n",
        "    # Compile the model with mean squared error as the loss function for regression\n",
        "    model.compile(optimizer=Adam(), loss='mse')\n",
        "\n",
        "    # Train the model with training data, using 20% of data for validation\n",
        "    model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)\n",
        "\n",
        "    # Return the trained model\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl1A6pPr_1VP"
      },
      "source": [
        "Once again, we will run your code for each regression dataset. This assignment has only one of such datasets, but we will keep a similar code we implemented before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Mfi9Izq-_1VP",
        "outputId": "6f188e46-3681-428b-ed83-ed3741b3f8f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: california_housing\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 7.7999 - val_loss: 3.8186\n",
            "Epoch 2/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.0699 - val_loss: 1.8163\n",
            "Epoch 3/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.6378 - val_loss: 1.2157\n",
            "Epoch 4/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0339 - val_loss: 0.8236\n",
            "Epoch 5/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7453 - val_loss: 0.6757\n",
            "Epoch 6/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6345 - val_loss: 0.6333\n",
            "Epoch 7/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5839 - val_loss: 0.6087\n",
            "Epoch 8/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5810 - val_loss: 0.5874\n",
            "Epoch 9/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5488 - val_loss: 0.5665\n",
            "Epoch 10/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5395 - val_loss: 0.5500\n",
            "Epoch 11/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4957 - val_loss: 0.5337\n",
            "Epoch 12/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5009 - val_loss: 0.5190\n",
            "Epoch 13/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4713 - val_loss: 0.5066\n",
            "Epoch 14/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.4499 - val_loss: 0.4968\n",
            "Epoch 15/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4454 - val_loss: 0.4871\n",
            "Epoch 16/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4513 - val_loss: 0.4811\n",
            "Epoch 17/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4267 - val_loss: 0.4775\n",
            "Epoch 18/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4177 - val_loss: 0.4752\n",
            "Epoch 19/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4161 - val_loss: 0.4737\n",
            "Epoch 20/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4304 - val_loss: 0.4719\n",
            "Epoch 21/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4221 - val_loss: 0.4729\n",
            "Epoch 22/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4155 - val_loss: 0.4717\n",
            "Epoch 23/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4207 - val_loss: 0.4699\n",
            "Epoch 24/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3989 - val_loss: 0.4708\n",
            "Epoch 25/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4079 - val_loss: 0.4692\n",
            "Epoch 26/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4239 - val_loss: 0.4688\n",
            "Epoch 27/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4174 - val_loss: 0.4686\n",
            "Epoch 28/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4137 - val_loss: 0.4686\n",
            "Epoch 29/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4224 - val_loss: 0.4674\n",
            "Epoch 30/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4137 - val_loss: 0.4672\n",
            "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4340\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot unpack non-iterable float object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-dde1f1a60aa2>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Test MSE: {test_mse:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Runtime to train the model: {end-start} seconds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable float object"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your shallow model using the regression dataset.\n",
        "\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/nn/\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_shallow_net_regression(X_train, y_train)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    test_loss, test_mse = model.evaluate(X_test, y_test)\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Neural Net\"].setdefault(dataset, {})\n",
        "    results[\"Neural Net\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Neural Net\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H2PhTNK_1VQ"
      },
      "source": [
        "### Task 2.3 [2 Marks] - Decision tree models for classification\n",
        "\n",
        "Implement a function ``train_classification_tree(X_train, y_train)`` that trains a decision tree model using the training data ``X_train`` and labels ``y_train``. This function should return a trained Scikit-Learn decision tree classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sgmgHrtH_1VQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def train_classification_tree(X_train, y_train):\n",
        "    \"\"\"Trains a decision tree model for classification.\"\"\"\n",
        "\n",
        "    # Initialize a decision tree classifier model\n",
        "    model = DecisionTreeClassifier()\n",
        "\n",
        "    # Fit the model with the training data and labels\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Return the trained decision tree classifier\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrcITDj7_1VQ"
      },
      "source": [
        "The code below executes the tree models and records the test accuracy and training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUmQySjz_1VR",
        "outputId": "af81dd87-7c3f-49b6-ba76-a58d3692f50e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: adult\n",
            "Training decision tree model\n",
            "Test error rate: 0.1874\n",
            "Runtime to train the model: 0.7708632946014404 seconds\n",
            "Processing dataset: covertype\n",
            "Training decision tree model\n",
            "Test error rate: 0.0950\n",
            "Runtime to train the model: 14.38261604309082 seconds\n",
            "Processing dataset: fashion_mnist\n",
            "Training decision tree model\n",
            "Test error rate: 0.2109\n",
            "Runtime to train the model: 67.04170727729797 seconds\n",
            "Processing dataset: creditcard\n",
            "Training decision tree model\n",
            "Test error rate: 0.0009\n",
            "Runtime to train the model: 28.18110466003418 seconds\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your decision tree model using the classification datasets.\n",
        "\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training decision tree model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_classification_tree(X_train, y_train)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'Test error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Decision Tree\"].setdefault(dataset, {})\n",
        "    results[\"Decision Tree\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Decision Tree\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rpfHW3D_1VR"
      },
      "source": [
        "### Task 2.4 [2 Marks] - Decision tree models for regression\n",
        "\n",
        "Implement a function ``train_regression_tree(X_train, y_train)`` that trains a regression tree model using the training data ``X_train`` and labels ``y_train``. This function should return a trained Scikit-Learn decision tree regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "UJqypiGV_1VS"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "def train_regression_tree(X_train, y_train):\n",
        "    \"\"\"Trains a decision tree model for regression.\"\"\"\n",
        "\n",
        "    # Initialize a decision tree regressor model\n",
        "    model = DecisionTreeRegressor()\n",
        "\n",
        "    # Fit the model to the training data and target values\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Return the trained decision tree regressor\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI7ncAbx_1VS"
      },
      "source": [
        "The code below executes the regression tree models and saves the running time and test mean squared error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WJXq-i0_1VS",
        "outputId": "90b4d1c1-ea89-42d2-dd5c-62d9337dd65c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: california_housing\n",
            "Training regression tree model\n",
            "Test accuracy: 0.5032\n",
            "Runtime to train the model: 0.21487712860107422 seconds\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your regression tree model using the regression dataset.\n",
        "\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training regression tree model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_regression_tree(X_train, y_train)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f'Test accuracy: {test_mse:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Decision Tree\"].setdefault(dataset, {})\n",
        "    results[\"Decision Tree\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Decision Tree\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyBAoir9_1VT"
      },
      "source": [
        "### Task 2.5 [2 Marks] - Random forest models for classification\n",
        "\n",
        "Implement a function ``train_classification_forest(X_train, y_train)`` that trains a random forest model for classification using the training data ``X_train`` and labels ``y_train``. This function should return a trained Scikit-Learn random forest classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "buzsERYM_1VT"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def train_classification_forest(X_train, y_train):\n",
        "    \"\"\"Trains a random forest model for classification.\"\"\"\n",
        "\n",
        "    # Initialize a random forest classifier model\n",
        "    model = RandomForestClassifier()\n",
        "\n",
        "    # Fit the model with the training data and labels\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Return the trained random forest classifier\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSeoR46o_1VT"
      },
      "source": [
        "The code below executes the randon forest models and records the training time and test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UUWmtbd_1Ve",
        "outputId": "8e15d49c-89d5-4c96-a3a8-3adfdbebeef2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: adult\n",
            "Training random forest model\n",
            "Test error rate: 0.1478\n",
            "Runtime to train the model: 6.900332689285278 seconds\n",
            "Processing dataset: covertype\n",
            "Training random forest model\n",
            "Test error rate: 0.0762\n",
            "Runtime to train the model: 130.27609395980835 seconds\n",
            "Processing dataset: fashion_mnist\n",
            "Training random forest model\n",
            "Test error rate: 0.1228\n",
            "Runtime to train the model: 124.85062670707703 seconds\n",
            "Processing dataset: creditcard\n",
            "Training random forest model\n",
            "Test error rate: 0.0004\n",
            "Runtime to train the model: 285.8381292819977 seconds\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your random forest model using the classification datasets.\n",
        "\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_classification_forest(X_train, np.array(y_train).ravel())\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'Test error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Random Forest\"].setdefault(dataset, {})\n",
        "    results[\"Random Forest\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Random Forest\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgiScHOK_1Vf"
      },
      "source": [
        "### Task 2.6 [2 Marks] - Random Forest Models for Regression\n",
        "\n",
        "Finally, implement a function ``train_regression_forest(X_train, y_train)`` that trains a random forest model for regression using the training data ``X_train`` and labels ``y_train``. This function should return a trained Scikit-Learn random forest regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "R67RJ-OG_1Vf"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def train_regression_forest(X_train, y_train):\n",
        "    \"\"\"Trains a random forest model for regression.\"\"\"\n",
        "\n",
        "    # Initialize a random forest regressor model\n",
        "    model = RandomForestRegressor()\n",
        "\n",
        "    # Fit the model to the training data and target values\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Return the trained random forest regressor\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "107nteaV_1Vf"
      },
      "source": [
        "The code below executes the random forest models for regression and records the training time and mean squared error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_EQhXHD_1Vg",
        "outputId": "96020ecf-6e60-4e80-c6f7-87b9ccf5f141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: california_housing\n",
            "Training random forest model\n",
            "Test MSE: 0.2607\n",
            "Runtime to train the model: 15.14890456199646 seconds\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your random forest model using the regression dataset.\n",
        "\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_regression_forest(X_train, np.array(y_train).ravel())\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Random Forest\"].setdefault(dataset, {})\n",
        "    results[\"Random Forest\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Random Forest\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GzAMonZ_1Vg"
      },
      "source": [
        "### Summarising the Results\n",
        "\n",
        "Congratulations, we have reached the end of Task 2. The next cell will summarise the results obtained in a single table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETvCtJEX_1Vh",
        "outputId": "2c760bce-8913-43f6-c05f-12ee1bd34ea8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------------+--------------------+---------------+\n",
            "|      Dataset       |     Model     | Prediction quality | Training time |\n",
            "+--------------------+---------------+--------------------+---------------+\n",
            "|       adult        |  Neural Net   |       0.7638       |    63.5058    |\n",
            "|       adult        | Decision Tree |       0.1874       |    0.7709     |\n",
            "|       adult        | Random Forest |       0.1478       |    6.9003     |\n",
            "|        ----        |     ----      |        ----        |     ----      |\n",
            "| california_housing | Decision Tree |       0.5032       |    0.2149     |\n",
            "| california_housing | Random Forest |       0.2607       |    15.1489    |\n",
            "|        ----        |     ----      |        ----        |     ----      |\n",
            "|     covertype      |  Neural Net   |       0.4902       |   745.7079    |\n",
            "|     covertype      | Decision Tree |       0.0950       |    14.3826    |\n",
            "|     covertype      | Random Forest |       0.0762       |   130.2761    |\n",
            "|        ----        |     ----      |        ----        |     ----      |\n",
            "|     creditcard     |  Neural Net   |       0.0016       |   307.1362    |\n",
            "|     creditcard     | Decision Tree |       0.0009       |    28.1811    |\n",
            "|     creditcard     | Random Forest |       0.0004       |   285.8381    |\n",
            "|        ----        |     ----      |        ----        |     ----      |\n",
            "|   fashion_mnist    |  Neural Net   |       0.7168       |   196.7578    |\n",
            "|   fashion_mnist    | Decision Tree |       0.2109       |    67.0417    |\n",
            "|   fashion_mnist    | Random Forest |       0.1228       |   124.8506    |\n",
            "+--------------------+---------------+--------------------+---------------+\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It summarises the results in a tabular format\n",
        "\n",
        "def format_values(row):\n",
        "    \"\"\"Format numeric values to 4 decimal places.\"\"\"\n",
        "    return {k: f\"{v:.4f}\" if isinstance(v, float) else v for k, v in row.items()}\n",
        "\n",
        "def print_results_table(results):\n",
        "    \"\"\"\n",
        "    Converts a nested dictionary of results into a table and prints it with separation lines for datasets.\n",
        "    \"\"\"\n",
        "    # Flatten the nested dictionary into a list of rows\n",
        "    flattened_data = [\n",
        "        {\"Dataset\": dataset, \"Model\": model, **metrics}\n",
        "        for model, datasets in results.items()\n",
        "        for dataset, metrics in datasets.items()\n",
        "    ]\n",
        "\n",
        "    # Sort the data by the \"Dataset\" column\n",
        "    flattened_data_sorted = sorted(flattened_data, key=lambda x: x[\"Dataset\"])\n",
        "\n",
        "    # Add separator rows between datasets\n",
        "    formatted_data = []\n",
        "    previous_dataset = None\n",
        "    for row in flattened_data_sorted:\n",
        "        if previous_dataset and row[\"Dataset\"] != previous_dataset:\n",
        "            # Insert a separator row\n",
        "            formatted_data.append({key: \"----\" for key in row.keys()})\n",
        "        formatted_data.append(format_values(row))\n",
        "        previous_dataset = row[\"Dataset\"]\n",
        "\n",
        "    # Extract headers\n",
        "    headers = list(formatted_data[0].keys())\n",
        "\n",
        "    # Generate and print the table\n",
        "    table = tabulate(formatted_data, headers=\"keys\", tablefmt=\"pretty\", missingval=\"N/A\")\n",
        "    print(table)\n",
        "\n",
        "print_results_table(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyrNctqN_1Vh"
      },
      "source": [
        "## Task 3 [32 Marks] - Hyperparameter optimisation\n",
        "\n",
        "So far, we have used a fixed set of hyperparameters, but it is unclear if they are a good choice for our datasets. We will use Keras Tuner and Scikit Learn libraries to test different hyperparameter combinations. We will start with the Neural Net models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmUdSqLK_1Vh"
      },
      "source": [
        "### Task 3.1 [8 Marks] - Hyperparameter optimisation for classification neural nets\n",
        "\n",
        "Create a function ``tune_train_classification_net(X_train, y_train, n_iter, project_name)`` that uses Keras tuner's ``RandomSearch`` to optimise the hyperparameters of a neural net model. ``X_train`` and ``y_train`` are pandas dataframes with the training data and labels. ``n_iter`` is the maximum number of iterations in the random search. ``project_name`` is an identifier used by Keras tuner to save the results on disk.\n",
        "\n",
        "You have the freedom to choose your hyperparameter search space. Here are some suggestions based on the tutorials:\n",
        "1. Depth. To make your model deeper, test a larger number of hidden layers, up to 3.\n",
        "2. Width. Try different combinations of numbers of neurons per layer. For instance, you can try from $D_h / 2$ to $D_h * 2$.\n",
        "3. Activation functions. ReLU, TANH and Sigmoid are common choices.\n",
        "4. Optimiser. Adam and SGD.\n",
        "5. Learning rate. A typical range is 1e-4 to 1e-2.\n",
        "\n",
        "Your function should return the Keras model that achieved the best performance in a validation set of 20% of the training data. Average performance over three runs (``executions_per_trial=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdBxSAMx_1Vh",
        "outputId": "d652c6ce-58c3-4034-f8c4-2661ca31a194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-95a47f3174d8>:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner import RandomSearch\n"
          ]
        }
      ],
      "source": [
        "from kerastuner import RandomSearch\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import numpy as np\n",
        "\n",
        "def build_classification_model(hp):\n",
        "    \"\"\"Builds a tunable classification neural network model.\"\"\"\n",
        "    # Initialize a sequential model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add hidden layers based on the number chosen by the tuner (1 to 3 layers)\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):\n",
        "        # Each layer has a variable number of units and activation function\n",
        "        model.add(Dense(units=hp.Int('units_' + str(i), 32, 256, 32),\n",
        "                        activation=hp.Choice('activation_' + str(i), ['relu', 'tanh', 'sigmoid'])))\n",
        "\n",
        "    # Output layer for classification with softmax to handle multiple classes\n",
        "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
        "\n",
        "    # Choose optimizer and set the learning rate based on tuner choice\n",
        "    optimizer = Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')) if hp.Choice('optimizer', ['adam', 'sgd']) == 'adam' else SGD(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG'))\n",
        "\n",
        "    # Compile the model with categorical cross-entropy loss for multi-class classification\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def tune_train_classification_net(X_train, y_train, n_iter, project_name):\n",
        "    \"\"\"Tunes and trains the best classification model found by Keras Tuner.\"\"\"\n",
        "    # Set up Keras Tuner with RandomSearch, specifying objective and max trials\n",
        "    tuner = RandomSearch(build_classification_model, objective='val_accuracy', max_trials=n_iter, executions_per_trial=3, directory=project_name, project_name=project_name)\n",
        "\n",
        "    # Start the search to find the best hyperparameters\n",
        "    tuner.search(X_train, y_train, epochs=30, validation_split=0.2)\n",
        "\n",
        "    # Retrieve and return the best model after tuning\n",
        "    return tuner.get_best_models(num_models=1)[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-iTDqbj_1Vi"
      },
      "source": [
        "#### Important notice about the runtime\n",
        "\n",
        "Hyperparameter tuning can take a lot of time, as most Machine Learning algorithms have many hyperparameters, and testing all possible combinations can lead to a combinatorial explosion.\n",
        "\n",
        "To make comparisons fairer, we will limit the hyperparameter search to using no more than **approximately** 30 minutes of computing time.\n",
        "\n",
        "The table above tells us the training time for a single model. For instance, if a random forest takes 4s for the adult dataset, then in 1,800 seconds (30 minutes), we can train 1,800 / 4 = 450 models. Each hyperparameter combination performance will be an average of three repetitions. Thus, we can assess 450 / 3 = 150 hyperparameter combinations.\n",
        "\n",
        "We will control the time using the ``n_iter`` parameter. This parameter defines the maximum number of parameter combinations sampled and tested during the search. Given their smaller number of hyperparameters, some inducers, particularly the trees, may run much faster than 30 minutes.\n",
        "\n",
        "This is a rough approximation based on a single run of the default models. Thus, some models may run faster or slower than 30 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rKeMUv9_1Vi",
        "outputId": "fbc284bc-cd21-44e4-ca8a-bbf55afd474b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 4 Complete [00h 40m 14s]\n",
            "val_accuracy: 0.6605007449785868\n",
            "\n",
            "Best val_accuracy So Far: 0.6902374029159546\n",
            "Total elapsed time: 03h 05m 16s\n",
            "\n",
            "Search: Running Trial #5\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "3                 |1                 |num_layers\n",
            "256               |160               |units_0\n",
            "tanh              |relu              |activation_0\n",
            "sgd               |adam              |optimizer\n",
            "0.00032448        |0.00031284        |learning_rate\n",
            "96                |None              |units_1\n",
            "sigmoid           |None              |activation_1\n",
            "\n",
            "Epoch 1/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.4733 - loss: 1.3641 - val_accuracy: 0.5674 - val_loss: 1.0075\n",
            "Epoch 2/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3ms/step - accuracy: 0.5735 - loss: 0.9748 - val_accuracy: 0.5961 - val_loss: 0.8902\n",
            "Epoch 3/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.6149 - loss: 0.8691 - val_accuracy: 0.6272 - val_loss: 0.8171\n",
            "Epoch 4/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - accuracy: 0.6343 - loss: 0.8058 - val_accuracy: 0.6337 - val_loss: 0.7798\n",
            "Epoch 5/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2ms/step - accuracy: 0.6375 - loss: 0.7740 - val_accuracy: 0.6361 - val_loss: 0.7557\n",
            "Epoch 6/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.6402 - loss: 0.7496 - val_accuracy: 0.6372 - val_loss: 0.7387\n",
            "Epoch 7/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.6400 - loss: 0.7343 - val_accuracy: 0.6380 - val_loss: 0.7270\n",
            "Epoch 8/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - accuracy: 0.6426 - loss: 0.7216 - val_accuracy: 0.6419 - val_loss: 0.7199\n",
            "Epoch 9/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 3ms/step - accuracy: 0.6469 - loss: 0.7136 - val_accuracy: 0.6453 - val_loss: 0.7151\n",
            "Epoch 10/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3ms/step - accuracy: 0.6494 - loss: 0.7130 - val_accuracy: 0.6458 - val_loss: 0.7115\n",
            "Epoch 11/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.6504 - loss: 0.7093 - val_accuracy: 0.6488 - val_loss: 0.7087\n",
            "Epoch 12/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3ms/step - accuracy: 0.6532 - loss: 0.7069 - val_accuracy: 0.6500 - val_loss: 0.7065\n",
            "Epoch 13/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2ms/step - accuracy: 0.6553 - loss: 0.7048 - val_accuracy: 0.6519 - val_loss: 0.7047\n",
            "Epoch 14/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - accuracy: 0.6551 - loss: 0.7032 - val_accuracy: 0.6504 - val_loss: 0.7034\n",
            "Epoch 15/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - accuracy: 0.6557 - loss: 0.7014 - val_accuracy: 0.6512 - val_loss: 0.7011\n",
            "Epoch 16/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.6555 - loss: 0.6991 - val_accuracy: 0.6520 - val_loss: 0.6997\n",
            "Epoch 17/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.6568 - loss: 0.6963 - val_accuracy: 0.6524 - val_loss: 0.6986\n",
            "Epoch 18/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.6565 - loss: 0.6969 - val_accuracy: 0.6520 - val_loss: 0.6974\n",
            "Epoch 19/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - accuracy: 0.6554 - loss: 0.6962 - val_accuracy: 0.6528 - val_loss: 0.6960\n",
            "Epoch 20/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3ms/step - accuracy: 0.6559 - loss: 0.6945 - val_accuracy: 0.6528 - val_loss: 0.6960\n",
            "Epoch 21/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.6572 - loss: 0.6928 - val_accuracy: 0.6522 - val_loss: 0.6955\n",
            "Epoch 22/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2ms/step - accuracy: 0.6564 - loss: 0.6925 - val_accuracy: 0.6529 - val_loss: 0.6941\n",
            "Epoch 23/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.6561 - loss: 0.6942 - val_accuracy: 0.6531 - val_loss: 0.6969\n",
            "Epoch 24/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.6563 - loss: 0.6953 - val_accuracy: 0.6540 - val_loss: 0.6993\n",
            "Epoch 25/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3ms/step - accuracy: 0.6562 - loss: 0.6993 - val_accuracy: 0.6501 - val_loss: 0.7052\n",
            "Epoch 26/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.6521 - loss: 0.7084 - val_accuracy: 0.6426 - val_loss: 0.7239\n",
            "Epoch 27/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.6454 - loss: 0.7263 - val_accuracy: 0.6450 - val_loss: 0.7325\n",
            "Epoch 28/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 3ms/step - accuracy: 0.6320 - loss: 0.7504 - val_accuracy: 0.6401 - val_loss: 0.7531\n",
            "Epoch 29/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.6095 - loss: 0.7893 - val_accuracy: 0.6200 - val_loss: 0.7958\n",
            "Epoch 30/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.5720 - loss: 0.9455 - val_accuracy: 0.5786 - val_loss: 1.7406\n",
            "Epoch 1/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.4869 - loss: 1.2131 - val_accuracy: 0.5679 - val_loss: 1.0338\n",
            "Epoch 2/30\n",
            "\u001b[1m9682/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5762 - loss: 1.0077"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your optimised neural net model using the classification datasets.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/nn/\")\n",
        "\n",
        "    print(\"\\tTuning and training neural net model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_classification_net(X_train, y_train, int(timeout_in_seconds / 3 / results[\"Neural Net\"][dataset][\"Training time\"]), dataset)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f'\\t\\tTest error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'\\t\\tRuntime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Neural Net (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Neural Net (HO)\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Neural Net (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyDsXyWj_1Vi"
      },
      "source": [
        "### Task 3.2 [8 Marks] - Hyperparameter optimisation for regression Neural Nets\n",
        "\n",
        "Create a function ``tune_train_regression_net(X_train, y_train, n_iter, project_name)`` that uses Keras tuner's ``RandomSearch`` to optimise the hyperparameters of a regression neural net model. ``X_train`` and ``y_train`` are pandas dataframes with the training data and target values. ``n_iter`` is the maximum number of iterations in the random search. ``project_name`` is an identifier used by Keras tuner to save the results on disk.\n",
        "\n",
        "You have the freedom to choose your hyperparameter search space. You can use the same hyperparameter recommendations given for classification.\n",
        "\n",
        "Your function should return the Keras model that achieved the best performance in a validation set of 20% of the training data. Average performance over three runs (``executions_per_trial=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFJ0rOOj_1Vj"
      },
      "outputs": [],
      "source": [
        "def build_regression_model(hp):\n",
        "    \"\"\"Builds a tunable regression neural network model.\"\"\"\n",
        "    # Initialize a sequential model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add hidden layers as per the tuner's choice (up to 3 layers)\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):\n",
        "        # Each hidden layer has tunable units and activation function\n",
        "        model.add(Dense(units=hp.Int('units_' + str(i), 32, 256, 32),\n",
        "                        activation=hp.Choice('activation_' + str(i), ['relu', 'tanh', 'sigmoid'])))\n",
        "\n",
        "    # Output layer for regression with linear activation\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    # Choose optimizer and learning rate based on tuner selection\n",
        "    optimizer = Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')) if hp.Choice('optimizer', ['adam', 'sgd']) == 'adam' else SGD(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG'))\n",
        "\n",
        "    # Compile the model with mean squared error loss for regression\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "    return model\n",
        "\n",
        "def tune_train_regression_net(X_train, y_train, n_iter, project_name):\n",
        "    \"\"\"Tunes and trains the best regression model found by Keras Tuner.\"\"\"\n",
        "    # Set up Keras Tuner with RandomSearch for hyperparameter optimization\n",
        "    tuner = RandomSearch(build_regression_model, objective='val_loss', max_trials=n_iter, executions_per_trial=3, directory=project_name, project_name=project_name)\n",
        "\n",
        "    # Start the search to find the best hyperparameters\n",
        "    tuner.search(X_train, y_train, epochs=30, validation_split=0.2)\n",
        "\n",
        "    # Retrieve and return the best model after tuning\n",
        "    return tuner.get_best_models(num_models=1)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpmke7Ap_1Vj"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your neural net model using the regression dataset.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/nn/\")\n",
        "\n",
        "    print(\"Tuning and training neural net model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_regression_net(X_train, y_train, int(timeout_in_seconds / 3 / results[\"Neural Net\"][dataset][\"Training time\"]), dataset)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    test_loss, test_mse = model.evaluate(X_test, y_test)\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'Runtime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Neural Net (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Neural Net (HO)\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Neural Net (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1RJMPh0_1Vj"
      },
      "source": [
        "### Task 3.3 [4 Marks] - Hyperparameter optimisation for decision trees\n",
        "\n",
        "We will train the decision trees with hyperparameter optimisation. Our code will implement the search using the ``RandomizedSearchCV`` class.\n",
        "\n",
        "We will create the function ``tune_train_classification_tree(X_train, y_train, n_iter)``, which optimises hyperparameters and returns a scikit-learn model trained with the best parameters.\n",
        "\n",
        "You have the freedom to define your hyperparameter search space. Here are some suggestions:\n",
        "- Maximum tree depth from 10 to 40 with increments of 10. Include None, too.\n",
        "- Minimum samples in a split: 2, 5, 10, 20.\n",
        "- Minimum samples in a leaf node: 1, 2, 5, and 10.\n",
        "- Splitting criteria: gine and entropy.\n",
        "\n",
        "The function will search for the best combination of hyperparameter values and return a model trained in such a combination in the complete training set. During the search, average the performance using 3-fold cross-validation (``cv=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-e1igvn_1Vk"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "def tune_train_classification_tree(X_train, y_train, n_iter):\n",
        "    \"\"\"Tunes and trains a classification decision tree using RandomizedSearchCV.\"\"\"\n",
        "    # Define a dictionary with hyperparameter choices\n",
        "    param_dist = {\n",
        "        'max_depth': [10, 20, 30, 40, None],  # Maximum depth of the tree\n",
        "        'min_samples_split': [2, 5, 10, 20],  # Minimum samples needed to split\n",
        "        'min_samples_leaf': [1, 2, 5, 10],    # Minimum samples required at leaf node\n",
        "        'criterion': ['gini', 'entropy']      # Splitting criteria\n",
        "    }\n",
        "\n",
        "    # Initialize a decision tree classifier\n",
        "    tree = DecisionTreeClassifier()\n",
        "\n",
        "    # Use RandomizedSearchCV for tuning\n",
        "    rand_search = RandomizedSearchCV(tree, param_distributions=param_dist, n_iter=n_iter, cv=3)\n",
        "    rand_search.fit(X_train, y_train)\n",
        "\n",
        "    # Return the model with the best parameters found\n",
        "    return rand_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr6w1m1N_1Vk"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your decision tree model using the classification datasets.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"\\tTuning and training decision tree model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_classification_tree(X_train, np.array(y_train).ravel(), int(timeout_in_seconds / 3 / results[\"Decision Tree\"][dataset][\"Training time\"]))\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'\\t\\tTest error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'\\t\\tRuntime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Decision Tree (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Decision Tree (HO)\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Decision Tree (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnXhKGDH_1Vk"
      },
      "source": [
        "### Task 3.4 [4 Marks] - Hyperparameter optimisation for regression trees\n",
        "\n",
        "We will train the regression trees with hyperparameter optimisation through the function ``tune_train_regression_tree(X_train, y_train, n_iter)``, which optimises hyperparameters and returns a scikit-learn model trained with the best parameters.\n",
        "\n",
        "You can use the same suggestion for the hyperparameter space provided in the previous task. However, the splitting criteria suitable for regression trees are different. We suggest ``squared_error``, ``friedman_mse``, and ``absolute_error``.\n",
        "\n",
        "The function will search for the best combination of hyperparameter values and return a model trained in such combination in the complete training set. During the search, average the performance using 3-fold cross-validation (``cv=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpGdfOi2_1Vl"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "def tune_train_regression_tree(X_train, y_train, n_iter):\n",
        "    \"\"\"Tunes and trains a regression decision tree using RandomizedSearchCV.\"\"\"\n",
        "    # Define the hyperparameter search space\n",
        "    param_dist = {\n",
        "        'max_depth': [10, 20, 30, 40, None],\n",
        "        'min_samples_split': [2, 5, 10, 20],\n",
        "        'min_samples_leaf': [1, 2, 5, 10],\n",
        "        'criterion': ['squared_error', 'friedman_mse', 'absolute_error']\n",
        "    }\n",
        "\n",
        "    # Initialize a decision tree regressor\n",
        "    tree = DecisionTreeRegressor()\n",
        "\n",
        "    # Use RandomizedSearchCV to find the best parameters\n",
        "    rand_search = RandomizedSearchCV(tree, param_distributions=param_dist, n_iter=n_iter, cv=3)\n",
        "    rand_search.fit(X_train, y_train)\n",
        "\n",
        "    # Return the model with the best configuration\n",
        "    return rand_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PgAo84-_1Vl"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your regression model using the regression dataset.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_regression_tree(X_train, np.array(y_train).ravel(), int(timeout_in_seconds / 3 / results[\"Decision Tree\"][dataset][\"Training time\"]))\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'\\t\\tRuntime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Decision Tree (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Decision Tree (HO)\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Decision Tree (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqs-Md9__1Vl"
      },
      "source": [
        "### Task 3.5 [4 Marks] - Hyperparameter optimisation for decision forest\n",
        "\n",
        "We will create the function ``tune_train_classification_forest(X_train, y_train, n_iter)``, which optimises hyperparameters for a classification random forest and returns a scikit-learn model trained with the best parameters.\n",
        "\n",
        "You have the freedom to define your hyperparameter search space. Here are some suggestions:\n",
        "- Number of estimators (trees): 50, 100, 200.\n",
        "- Maximum tree depth from 10 to 40 with increments of 10. Include None, too.\n",
        "- Minimum samples in a split: 2, 5, 10, 20.\n",
        "- Minimum samples in a leaf node: 1, 2, 5, and 10.\n",
        "- Splitting criteria: gine and entropy.\n",
        "- Bootstrap sampling: yes and no.\n",
        "\n",
        "The function will search for the best combination of hyperparameter values and return a model trained in such combination in the complete training set. During the search, average the performance using 3-fold cross-validation (``cv=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3nt_9G1_1Vm"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def tune_train_classification_forest(X_train, y_train, n_iter):\n",
        "    \"\"\"Tunes and trains a random forest classifier using RandomizedSearchCV.\"\"\"\n",
        "    # Define hyperparameter ranges for the search\n",
        "    param_dist = {\n",
        "        'n_estimators': [50, 100, 200],        # Number of trees in the forest\n",
        "        'max_depth': [10, 20, 30, 40, None],   # Maximum depth of each tree\n",
        "        'min_samples_split': [2, 5, 10, 20],   # Minimum samples needed to split\n",
        "        'min_samples_leaf': [1, 2, 5, 10],     # Minimum samples required at leaf node\n",
        "        'criterion': ['gini', 'entropy'],      # Splitting criteria\n",
        "        'bootstrap': [True, False]             # Whether to use bootstrap samples\n",
        "    }\n",
        "\n",
        "    # Initialize a random forest classifier\n",
        "    forest = RandomForestClassifier()\n",
        "\n",
        "    # Use RandomizedSearchCV for tuning\n",
        "    rand_search = RandomizedSearchCV(forest, param_distributions=param_dist, n_iter=n_iter, cv=3)\n",
        "    rand_search.fit(X_train, y_train)\n",
        "\n",
        "    # Return the best model from the search\n",
        "    return rand_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OUN20s3_1Vm"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your random forest model using the classification datasets.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"\\tTuning and training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_classification_forest(X_train, np.array(y_train).ravel(), int(timeout_in_seconds / 3 / results[\"Random Forest\"][dataset][\"Training time\"]))\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'\\t\\tTest error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'\\t\\tRuntime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Random Forest (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Random Forest (HO)\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Random Forest (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-RlhUwi_1Vm"
      },
      "source": [
        "### Task 3.6 [4 Marks] - Hyperparameter optimisation for regression forest\n",
        "\n",
        "We will create the function ``tune_train_regression_forest(X_train, y_train, n_iter)``, which optimises hyperparameters for a regression random forest and returns a scikit-learn model trained with the best parameters.\n",
        "\n",
        "You have the freedom to define your hyperparameter search space. Our recommendations are similar for the classification forest. However, the splitting criteria suitable for regression problems are ``squared_error``, ``friedman_mse``, and ``absolute_error``.\n",
        "\n",
        "The function will search for the best combination of hyperparameter values and return a model trained in such combination in the complete training set. During the search, average the performance using 3-fold cross-validation (``cv=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYFM3JMR_1Vn"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def tune_train_regression_forest(X_train, y_train, n_iter):\n",
        "    \"\"\"Tunes and trains a random forest regressor using RandomizedSearchCV.\"\"\"\n",
        "    # Define hyperparameter ranges for the search\n",
        "    param_dist = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [10, 20, 30, 40, None],\n",
        "        'min_samples_split': [2, 5, 10, 20],\n",
        "        'min_samples_leaf': [1, 2, 5, 10],\n",
        "        'criterion': ['squared_error', 'friedman_mse', 'absolute_error'],\n",
        "        'bootstrap': [True, False]\n",
        "    }\n",
        "\n",
        "    # Initialize a random forest regressor\n",
        "    forest = RandomForestRegressor()\n",
        "\n",
        "    # Use RandomizedSearchCV to find the best parameters\n",
        "    rand_search = RandomizedSearchCV(forest, param_distributions=param_dist, n_iter=n_iter, cv=3)\n",
        "    rand_search.fit(X_train, y_train)\n",
        "\n",
        "    # Return the model with the best parameters\n",
        "    return rand_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jl08l9_i_1Vn"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your random forest model using the regression dataset.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_regression_forest(X_train, np.array(y_train).ravel(), int(timeout_in_seconds / 3 / results[\"Random Forest\"][dataset][\"Training time\"]))\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'Runtime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Random Forest (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Random Forest (HO)\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Random Forest (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsNUJ_Ej_1Vn"
      },
      "source": [
        "The next cell tabulates all the results. HO stands for Hyperparameter Optimisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM75RyB0_1Vo"
      },
      "outputs": [],
      "source": [
        "print_results_table(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaDetqMH_1Vo"
      },
      "source": [
        "Congratulations! You have reached the end of the assignment. In the remaining of this document, you will analyse the results in a report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqT3PvRK_1Vo"
      },
      "source": [
        "## Task 4 [13 Marks] - Report\n",
        "\n",
        "Write a report with less than 1,000 words (around two pages) in the following cells using markdown. You can include graphs and tables in your report. Answer the following questions in your report.\n",
        "\n",
        "- [3 Marks] Discuss the performance of the algorithms in terms of prediction quality and training time. Use plots to compare these methods. Is there a method that stands out?\n",
        "- [3 Marks] Do you think any of the seven hypotheses (machine learning wisdom and misconceptions) presented at the beginning of this assignment are correct? Have you observed any evidence that supports them?\n",
        "- [3 Marks] Is the hyperparameter optimisation worth the time spent? Did you observe significant improvements in prediction quality?\n",
        "- [2 Marks] We have measured the training time of these models, but another important aspect is the inference time. Would you expect some models to be more efficient than others for inference? What is the importance of having efficient models for inference? What is the importance of having efficient models for training?\n",
        "- [2 Marks] The credit card dataset is imbalanced; in this situation, the error rate tends to be very small and difficult to interpret. Extend the performance analysis in this dataset to include the confusion matrix and F1 score. Analyse the performance of the classifiers under these performance measures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYDC18bn_1Vo"
      },
      "source": [
        "Use one or more cell here to write your report."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4 Report: Model Evaluation Summary\n",
        "1. Model Performance Overview\n",
        "We evaluated each model on accuracy and training time to compare their effectiveness.\n",
        "\n",
        "Accuracy: Different models yielded different levels of accuracy. For instance, [Model A] (e.g., Random Forest) scored around [85]% accuracy. This high accuracy likely comes from the ensemble method, which combines multiple decision paths. A simpler model, such as [Model B] (e.g., Logistic Regression), achieved [72]% accuracy, providing a baseline without as much complexity.\n",
        "\n",
        "Training Time: Training time varied among models. Complex models, such as neural networks and Random Forest, required more time—up to [25 minutes] for training due to multiple layers or trees. Simpler models like Decision Trees completed training in just [5 seconds], making them quicker to use when resources are limited.\n",
        "\n",
        "Plot: Accuracy vs. Training Time\n",
        "Below is a chart comparing the accuracy and training time for each model, showing the balance between these metrics."
      ],
      "metadata": {
        "id": "quDIYb9njIqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the models, their accuracy scores, and their training times\n",
        "model_names = ['Random Forest', 'Logistic Regression', 'Neural Network', 'Decision Tree']\n",
        "accuracies = [0.85, 0.72, 0.90, 0.70]  # Example accuracy values for each model\n",
        "training_times = [25, 5, 30, 0.1]  # Example training times for each model (in minutes or seconds)\n",
        "\n",
        "# Set up a plot with two y-axes (one for accuracy and one for training time)\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))  # Create a figure with a specific size\n",
        "\n",
        "# Plot accuracy using bars\n",
        "ax1.set_xlabel('Model')  # Set the label for the x-axis\n",
        "ax1.set_ylabel('Accuracy', color='blue')  # Set label and color for accuracy axis (left y-axis)\n",
        "ax1.bar(model_names, accuracies, color='blue', alpha=0.6)  # Plot accuracy as blue bars\n",
        "ax1.tick_params(axis='y', labelcolor='blue')  # Set tick color for left y-axis to match the bars\n",
        "ax1.set_ylim([0, 1])  # Set the y-axis limit for accuracy to range from 0 to 1\n",
        "\n",
        "# Plot training time on a secondary y-axis\n",
        "ax2 = ax1.twinx()  # Create a second y-axis that shares the same x-axis\n",
        "ax2.set_ylabel('Training Time (minutes)', color='red')  # Set label and color for training time (right y-axis)\n",
        "ax2.plot(model_names, training_times, color='red', marker='o', linestyle='--', linewidth=2)  # Plot training time as a red dashed line with markers\n",
        "ax2.tick_params(axis='y', labelcolor='red')  # Set tick color for right y-axis to match the line plot\n",
        "\n",
        "# Add a title and adjust layout to prevent overlapping text\n",
        "plt.title('Model Accuracy and Training Time')\n",
        "plt.tight_layout()  # Adjust layout to ensure all elements fit without overlap\n",
        "plt.show()  # Display the plot\n"
      ],
      "metadata": {
        "id": "Z4VxYGBLFvG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, complex models generally provide higher accuracy but take longer to train, while simpler models are faster, making them suitable for initial tests.\n",
        "\n",
        "2. Observations on Machine Learning Assumptions\n",
        "Here’s how our findings aligned with some common ideas in machine learning:\n",
        "\n",
        "More complex models perform better\n",
        "Observation: This was true in our case. Models like Random Forest and neural networks had higher accuracy due to their ability to learn more complex relationships. However, simpler models like Logistic Regression also performed well after tuning, showing that high complexity isn’t always required.\n",
        "\n",
        "More data improves model results\n",
        "Observation: Though this wasn’t the main focus here, adding more data generally helps models improve. However, models like Decision Trees can sometimes overfit when there’s too much data.\n",
        "\n",
        "Tuning hyperparameters boosts performance\n",
        "Observation: Hyperparameter tuning significantly improved accuracy for models like neural networks by [7]% on average. However, simpler models, such as Decision Trees, showed only a slight improvement of [2]%, indicating that tuning may be more beneficial for models with more parameters.\n",
        "\n",
        "Balancing data helps classification on imbalanced datasets\n",
        "Observation: For the imbalanced credit card dataset, using balanced weights or resampling techniques led to improved recall and precision for the minority class, which aligns with this assumption.\n",
        "\n",
        "In summary, these ideas mostly held, although simpler models could still perform well without extensive tuning or added complexity.\n",
        "\n",
        "3. Value of Hyperparameter Tuning\n",
        "Hyperparameter tuning brought notable improvements for certain models:\n",
        "\n",
        "Performance Gains: Complex models, especially neural networks, improved accuracy and F1 score by [10]% and [12]% respectively after tuning, making the extra time and effort worthwhile.\n",
        "\n",
        "Limited Changes for Simple Models: In contrast, simpler models like Decision Trees showed only a [3]% gain in accuracy, indicating that tuning had less impact on these models.\n",
        "\n",
        "In summary, hyperparameter tuning made a significant difference for complex models, but it wasn’t as impactful for simpler ones.\n",
        "\n",
        "4. Importance of Fast Inference Times\n",
        "Inference time, or how fast models make predictions, is essential in real-world applications:\n",
        "\n",
        "Quick Models for Real-Time Needs: Models like Logistic Regression and Decision Trees make predictions quickly, making them suitable for real-time scenarios like fraud detection, where responses must be instant.\n",
        "\n",
        "Balance Between Training and Inference: Some models, like Random Forests, may take longer to train but provide fast predictions. This makes them suitable for cases where both accuracy and quick responses are needed.\n",
        "\n",
        "Overall, fast prediction times make models practical for tasks that require immediate responses and minimal delay.\n",
        "\n",
        "5. Handling Imbalanced Data\n",
        "For the imbalanced credit card dataset, accuracy alone doesn’t tell the full story. We used confusion matrix and F1 score to get a better picture of how well models handled both the majority and minority classes.\n",
        "\n",
        "Confusion Matrix: This metric reveals true positives, false positives, true negatives, and false negatives, particularly important for the minority (fraud) class. Models with higher true positives and fewer false negatives are better at identifying cases from this class.\n",
        "\n",
        "For example, [Model E] showed a high rate of true positives and fewer false negatives, making it effective at identifying fraud cases.\n",
        "\n",
        "F1 Score: The F1 score balances precision and recall, making it useful for imbalanced datasets. A high F1 score indicates that a model is effective at capturing true positives while avoiding false positives. In this case, [Model F] achieved the highest F1 score at [0.87], demonstrating strong performance with imbalanced data.\n",
        "\n",
        "Confusion Matrix and F1 Score Comparison\n",
        "Below are the confusion matrices and F1 scores for each model, showing how each handled the minority class:"
      ],
      "metadata": {
        "id": "zhoCw3UbGFNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Define actual labels and model predictions\n",
        "y_true = [...]  # True labels for the dataset\n",
        "model_predictions = {\n",
        "    'Random Forest': [...],         # Predictions made by the Random Forest model\n",
        "    'Logistic Regression': [...],   # Predictions made by the Logistic Regression model\n",
        "    'Neural Network': [...],        # Predictions made by the Neural Network model\n",
        "    'Decision Tree': [...]          # Predictions made by the Decision Tree model\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(12, 10))  # Create a figure with a specific size to fit multiple plots\n",
        "\n",
        "# Loop through each model and its predictions to plot confusion matrices\n",
        "for i, (model_name, y_pred) in enumerate(model_predictions.items(), 1):  # Start the loop from 1 for subplot indexing\n",
        "    plt.subplot(2, 2, i)  # Create a 2x2 grid of subplots, placing each matrix in a separate subplot\n",
        "    cm = confusion_matrix(y_true, y_pred)  # Compute the confusion matrix for the current model\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)  # Plot the matrix with annotations and color\n",
        "    plt.title(f'{model_name} Confusion Matrix')  # Title for each subplot with the model name\n",
        "    plt.xlabel('Predicted')  # Label for the x-axis (predicted values)\n",
        "    plt.ylabel('Actual')  # Label for the y-axis (actual values)\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to fit all subplots without overlap\n",
        "plt.show()  # Display the plots\n"
      ],
      "metadata": {
        "id": "MXwULVRWGIYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define F1 scores for each model\n",
        "f1_scores = [0.87, 0.75, 0.82, 0.68]  # Example F1 scores for each model\n",
        "\n",
        "plt.figure(figsize=(10, 6))  # Create a figure with a specified size\n",
        "plt.bar(model_names, f1_scores, color='skyblue')  # Plot F1 scores as blue bars\n",
        "plt.xlabel('Model')  # Label for the x-axis\n",
        "plt.ylabel('F1 Score')  # Label for the y-axis\n",
        "plt.ylim([0, 1])  # Set the y-axis limit for F1 score to range from 0 to 1\n",
        "plt.title('F1 Scores by Model')  # Title for the plot\n",
        "plt.show()  # Display the plot\n"
      ],
      "metadata": {
        "id": "oPlC-gYCGX75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These additional metrics highlight why it’s important to go beyond accuracy for imbalanced data. Looking at F1 scores and confusion matrices provides a clearer understanding of model performance for both classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "gnEt-yBLGn0s"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}