{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryan5859/Python-Stock-Project-Coursera/blob/main/COMP9414-Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8Em327q_1U6"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/UNSW-COMP9414/Assignment2/blob/main/COMP9414-Assignment2.ipynb)\n",
        "\n",
        "# COMP9414 24T3 - Assignment 2 - Neural Networks, Decision Trees and Random Forests\n",
        "\n",
        "## UNSW Sydney\n",
        "\n",
        "Designed by Gustavo Batista.\n",
        "\n",
        "Last change: 20th October, 2024."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW-Mfui4_1U8"
      },
      "source": [
        "Student name - zID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZzjIq5z_1U9"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "**Submission deadline:** Friday, 8th November 2024, at 17:00:00 AEDT.\n",
        "\n",
        "**Submission:** You can submit your solution via the give system using the command ``give cs9414 ass2 ass2.ipynb``.\n",
        "\n",
        "**Instructions:**\n",
        "* This is an **individual** assignment.\n",
        "* Write your name and zID on the top of this Jupyter Notebook.\n",
        "* You can only use the libraries listed in this notebook\n",
        "* You can reuse any piece of source code developed in the tutorials.\n",
        "* Do not modify the existing code in this notebook except to answer the questions. The cells that should be modified are indicated.\n",
        "* If you want to submit additional code (e.g., for generating plots), write it at the end of the notebook.\n",
        "* This notebook is worth **75** marks and will be rescaled to **25** marks.\n",
        "\n",
        "**Late Submission Policy:** A 5% reduction of the assignment value (i.e. 1.25 marks) will be applied per day for late submissions. For example, if an assignment gets an on-time mark of $20/25$ but is submitted three days late, the penalty will be $3*1.25 = 3.75$, so the final mark will be $16.25$. After five days, the assignment total mark will be reduced to 0 ($100\\%$ reduction). An assignment is considered one day late if submitted any time after the submission deadline, up to 24 hours past it.\n",
        "\n",
        "**Plagiarism:**\n",
        "\n",
        "Remember that ALL work submitted for this assignment must be your own work, and no sharing or copying of code or answers is allowed. You may discuss the assignment with other students but must not collaborate to develop answers to the questions. You may use code from the Internet only with suitable attribution of the source. You may not use ChatGPT or any similar software to generate any part of your explanations, evaluations or code. Do not use public code repositories on sites such as GitHub or file-sharing sites such as Google Drive to save any part of your work &ndash; make sure your code repository or cloud storage is private, and do not share any links. This also applies after you have finished the course, as we do not want next year’s students accessing your solution, and plagiarism penalties can still apply after the course has finished.\n",
        "\n",
        "All submitted assignments will be run through plagiarism detection software to detect similarities to other submissions, including from past years. You should **carefully** read the UNSW policy on academic integrity and plagiarism (linked from the course web page), noting, in particular, that collusion (working together on an assignment or sharing parts of assignment solutions) is a form of plagiarism.\n",
        "\n",
        "Finally, do not use any contract cheating “academies” or online “tutoring” services. This counts as serious misconduct with heavy penalties up to automatic failure of the course with 0 marks and expulsion from the university for repeat offenders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAuY8Y1b_1U_"
      },
      "source": [
        "## Technical prerequisites\n",
        "\n",
        "These are the libraries you are allowed to use. No other libraries will be accepted. Make sure you are using Python 3."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List of all necessary libraries\n",
        "libraries = [\n",
        "    # General Purpose\n",
        "    \"numpy\", \"pandas\", \"scipy\", \"matplotlib\", \"seaborn\", \"requests\",\n",
        "    \"beautifulsoup4\", \"pytest\", \"pillow\",\n",
        "\n",
        "    # Machine Learning\n",
        "    \"scikit-learn\", \"xgboost\", \"lightgbm\", \"catboost\", \"statsmodels\",\n",
        "\n",
        "    # Deep Learning and AI\n",
        "    \"tensorflow\", \"keras\", \"torch\", \"transformers\", \"opencv-python\",\n",
        "    \"nltk\", \"spacy\", \"gym\", \"tqdm\",\n",
        "\n",
        "    # Data Science and Visualization\n",
        "    \"plotly\", \"dash\", \"bokeh\", \"geopandas\", \"networkx\",\n",
        "\n",
        "    # Utilities\n",
        "    \"joblib\", \"dill\", \"dataclasses\", \"pydantic\",\"tabulate\",\"keras-tuner\",\"keras_tuner\"\n",
        "]\n",
        "\n",
        "# Install all libraries\n",
        "for lib in libraries:\n",
        "    os.system(f\"pip install {lib}\")\n",
        "\n",
        "print(\"All libraries have been installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds10JRoB_9Pj",
        "outputId": "3e099b28-e066-46ba-c87b-ec94f9cfde60"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries have been installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A48JHg_7_1VB"
      },
      "outputs": [],
      "source": [
        "# These are the allowed libraries. You can add other libraries used in the tutorials.\n",
        "\n",
        "# Common Python libraries\n",
        "import math\n",
        "import copy\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import matplotlib as mp\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# Scikit-Learn libraries for data preprocessing and model assessment\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Libraries for the tree models\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "\n",
        "# Scikit-learn libraries for hyperparameter tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Tensorflow/keras libraries for shallow and deep-learning models\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "\n",
        "# Keras Tuner libraries for hyperparameter tuning\n",
        "from keras_tuner import HyperModel\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "\n",
        "# Libraries to present results in tabular format\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ycN2xyQ_1VC"
      },
      "source": [
        "This assignment compares three Machine Learning approaches: Neural Networks, Decision Trees, and Random Forests. We will assess these approaches in five benchmark datasets with diverse characteristics.\n",
        "\n",
        "We would like to test a few hypotheses based on common Machine Learning wisdom and misconceptions.\n",
        "\n",
        "1. Neural networks are the best general classifiers regarding prediction quality (accuracy, error rate, precision, recall, etc.).\n",
        "2. Neural networks are time-consuming for training as fitting model parameters is slow and has many hyperparameters.\n",
        "3. Random forests are an excellent compromise between classification performance and hyperparameter tuning. They can often provide competitive accuracy without requiring much hyperparameter tuning.\n",
        "4. Neural networks are data-hungry and perform poorly in small datasets.\n",
        "5. Decision trees offer model interpretability but are not competitive in accuracy.\n",
        "6. Neural networks are the best models when learning from unstructured data such as images.\n",
        "7. Random forests are the best models when learning from structured data such as a tabular dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55kJes3q_1VD"
      },
      "source": [
        "## Task 0 - Datasets description, downloading and loading the data into a Pandas dataframe\n",
        "\n",
        "We have selected five publicly available benchmark datasets:\n",
        "\n",
        "1. **UCI adult income dataset.** This is a binary classification dataset in which we want to predict if a person earns more than $50k/year. It is a mid-size dataset (48K examples) with 14 features of mixed data types (categorical and continuous) with missing values.\n",
        "\n",
        "2. **Forest cover type dataset.** This is a large multi-class dataset with 580K examples and 54 features of mixed types. The objective is to predict the type of forest cover based on features such as soil type, elevation, and slope.\n",
        "\n",
        "3. **California housing prices**. This is a regression problem in which we want to predict housing prices based on numerical features, such as population, median income and location. It has 20K instances and nine features.\n",
        "\n",
        "4. **Fashion MNIST dataset**. It is an image classification dataset that is very similar to MNIST. Images are $28 \\times 28$ grayscale pixels. The objective is to classify ten different types of clothing. It has 60k training and 10K test instances.\n",
        "\n",
        "5. **Credit card fraud detection**. This is a binary classification dataset for detecting fraudulent transactions in credit card data. It is highly imbalanced, meaning that most transactions are normal, with some rare fraud cases. It has 284K instances and 30 numerical features.\n",
        "\n",
        "This table summarises the datasets.\n",
        "\n",
        "| Dataset                          | Problem Type        | Feature Type                          | Size        | Notable Challenge                                    |\n",
        "|-----------------------------------|---------------------|---------------------------------------|-------------|------------------------------------------------------|\n",
        "| **UCI Adult Income**              | Binary Classification| Categorical and Numerical             | 48,000      | Mix of feature types with missing values           |\n",
        "| **Forest Cover Type**             | Multi-class Classification | Categorical and Numerical       | 580,000     | Large dataset with mix of feature types      |\n",
        "| **California Housing Prices**     | Regression          | Numerical                              | 20,000      | Regression task      |\n",
        "| **Fashion MNIST**                 | Multi-class Classification (Image)| Image (grayscale)        | 60,000      | Weak features in the form of individual pixels brightness       |\n",
        "| **Credit Card Fraud Detection**   | Binary Classification (Imbalanced)| Numerical                | 284,000     | Highly imbalanced dataset        |\n",
        "\n",
        "Let's start by downloading the data from GitHub. The cell below will download and save the data into a local ``data`` folder. We will use the data later to train and assess our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klEb-SME_1VE",
        "outputId": "c7cf8756-7dfd-4d02-8702-e55d58279b90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading adult.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/adult/adult.zip...\n",
            "Downloading covertype.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/covertype/covertype.zip...\n",
            "Downloading california_housing.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/california_housing/california_housing.zip...\n",
            "Downloading creditcard.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/creditcard/creditcard.zip...\n",
            "Downloading fashion_mnist.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/fashion_mnist/fashion_mnist.zip...\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It downloads and unzips the datasets to your local disk.\n",
        "\n",
        "def download_and_extract(url, extract_to):\n",
        "    \"\"\"\n",
        "    Download a zip file from the URL and extract it to the specified directory.\n",
        "\n",
        "    Parameters:\n",
        "    - url (str): The URL of the zip file to download.\n",
        "    - extract_to (str): The directory where the zip file's contents will be extracted.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Get the file, dataset names from the URL\n",
        "    zip_filename = url.split(\"/\")[-1]\n",
        "    dataset_name = zip_filename.split(\".\")[0]\n",
        "\n",
        "    # Each dataset will have its folder\n",
        "    extract_to = extract_to + \"/\" + dataset_name\n",
        "\n",
        "    # Download the zip file\n",
        "    print(f\"Downloading {zip_filename} from {url}...\")\n",
        "    response = requests.get(url)\n",
        "    with open(zip_filename, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "    # Create the extraction directory if it doesn't exist\n",
        "    if not os.path.exists(extract_to):\n",
        "        os.makedirs(extract_to)\n",
        "\n",
        "    # Unzip the file\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "    # Remove the zip files\n",
        "    os.remove(zip_filename)\n",
        "\n",
        "# These are the URLs to the datasets. We have hosted the data on GitHub.\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/adult/adult.zip\",\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/covertype/covertype.zip\",\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/california_housing/california_housing.zip\",\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/creditcard/creditcard.zip\",\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/fashion_mnist/fashion_mnist.zip\",\n",
        "]\n",
        "\n",
        "for i, url in enumerate(urls):\n",
        "    download_and_extract(url, \"data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdovQ_MR_1VG"
      },
      "source": [
        "### Loading data into pandas\n",
        "\n",
        "The datasets are well-diversified in size (number of examples and features), number of class labels, feature types (continuous and discrete), class distribution, and presence of missing data.\n",
        "\n",
        "All datasets have pre-defined training and testing splits. We will use the training set to train the models and choose hyperparameters. You may further split the training set into training and validation sets. The test set should only be used to assess and compare the models.\n",
        "\n",
        "The next cell has a supporting function that loads a specified dataset training and test sets into a pandas' dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1ZuE8km_1VH",
        "outputId": "c6569979-a208-427d-9b5b-160953b47a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data/adult...\n",
            "Train Features Shape: (32561, 14), Train Labels Shape: (32561, 1)\n",
            "Test Features Shape: (16281, 14), Test Labels Shape: (16281, 1)\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It is a helper function that loads data from this into a Pandas dataframe.\n",
        "\n",
        "def load_train_test_data(path):\n",
        "    \"\"\"\n",
        "    Loads the train and test CSV files and returns them split into features (X) and labels (y).\n",
        "\n",
        "    Parameters:\n",
        "    - path (str): Path to the train and test CSV files.\n",
        "\n",
        "    Returns:\n",
        "    - X_train (DataFrame): Features of the training dataset.\n",
        "    - y_train (DataFrame): Labels of the training dataset.\n",
        "    - X_test (DataFrame): Features of the test dataset.\n",
        "    - y_test (DataFrame): Labels of the test dataset.\n",
        "    \"\"\"\n",
        "    # Load the training and testing data\n",
        "    train_df = pd.read_csv(f\"{path}/train.csv\")\n",
        "    test_df = pd.read_csv(f\"{path}/test.csv\")\n",
        "\n",
        "    # Select class label columns (those starting with 'Target')\n",
        "    y_train = train_df.filter(regex='^Target')\n",
        "    y_test = test_df.filter(regex='^Target')\n",
        "\n",
        "    # Select feature columns (all columns except the ones with 'Target' prefix)\n",
        "    X_train = train_df.drop(columns=y_train.columns)\n",
        "    X_test = test_df.drop(columns=y_test.columns)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Example usage:\n",
        "path = \"data/adult\"\n",
        "print(f\"Loading {path}...\")\n",
        "X_train, y_train, X_test, y_test = load_train_test_data(path)\n",
        "\n",
        "print(f\"Train Features Shape: {X_train.shape}, Train Labels Shape: {y_train.shape}\")\n",
        "print(f\"Test Features Shape: {X_test.shape}, Test Labels Shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mp_9v2i2_1VI"
      },
      "source": [
        "## Task 1 [14 Marks] - Data preprocessing\n",
        "\n",
        "Your first task is to preprocess the datasets. Preprocessing usually involves data cleaning and transformation to improve data quality and prepare the data for the specific requirements of the learning approaches.\n",
        "\n",
        "For data preparation, we have the following tasks:\n",
        "1. **Missing imputation (all models)**: The adult dataset has missing values, and none of our learning algorithm implementations can directly handle missing data. Two missing data treatments are eliminating the rows with missing data or replacing the missing values with estimated ones. *Mean imputation*, as the name suggests, replaces missing values with the attribute mean, median (continuous features) or mode (discrete features). These statistics must only be estimated in the training set.\n",
        "2. **Feature encoding (all models)**: Neural networks, tree and random forest implementations available in the Scikit-Learn library do not handle categorical attributes directly. Therefore, these attributes need to be converted into numerical attributes. Although several encoding approaches exist, we will use one-hot encoding, as it is simple and recommended for categorical features with a small cardinality.\n",
        "3. **Class attribute encoding (neural networks only)**: Neural networks also need a one-hot encoding for the class attribute. This step is not necessary for the tree models.\n",
        "4. **Rescaling attribute values (neural networks only)**: The neural network's training benefits from rescaling the attribute values. In this task, we will convert each attribute to a number in the 0 to 1 range by using a simple linear rescaling: $x_s = \\frac{x-min_f}{max_f-min_f}$, where $x_s$ is the recalled $x$ value, $min_f$ is the minimum and $max_f$ the maximum values for feature $f$ in the training data.\n",
        "\n",
        "Tree models typically do not use class encoding and rescaling. The reason is twofold: first, this preprocessing does not help these models fit better parameters; second, tree models are known for their interpretability, and these manipulations create models that are not easier and often harder to understand. Feature encoding is also unnecessary for many tree model implementations, including the well-known [XGBoost](https://xgboost.readthedocs.io/en/stable/) and [LightGBM](https://lightgbm.readthedocs.io/en/stable/). Unfortunately, the Scikit-Learn implementation of tree models does not support categorical attributes.\n",
        "\n",
        "**Warning**: Leaking information from the test set to the training set, even if such information is aggregated data such as means, maximums, and minimums, is considered a serious methodological error. For instance, mean imputation should use the mean only in the training set. Similarly, the maximum and minimum for attribute rescaling should be calculated in the training set. Consequently, we may see values outside the range of 0-1 in the rescaled test set. This mimics the situation in which we find extreme values after the model deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mderMHuE_1VI"
      },
      "source": [
        "### Task 1.1 [6 Marks] - Missing data removal or imputation\n",
        "\n",
        "Create a function ``missing_data(X_train, X_test)`` that imputes missing values in the dataframes `X_train` and `X_test`. When the function returns, both dataframes should have no missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NlbuPckb_1VJ"
      },
      "outputs": [],
      "source": [
        "def missing_data(X_train, X_test):\n",
        "    \"\"\"Fill missing values in train and test datasets.\"\"\"\n",
        "    # Fill numerical columns with median values\n",
        "    for col in X_train.select_dtypes(include=['float64', 'int64']).columns:\n",
        "        median_val = X_train[col].median()\n",
        "        X_train[col].fillna(median_val, inplace=True)\n",
        "        X_test[col].fillna(median_val, inplace=True)\n",
        "\n",
        "    # Fill categorical columns with mode values\n",
        "    for col in X_train.select_dtypes(include=['object']).columns:\n",
        "        mode_val = X_train[col].mode()[0]\n",
        "        X_train[col].fillna(mode_val, inplace=True)\n",
        "        X_test[col].fillna(mode_val, inplace=True)\n",
        "\n",
        "    return X_train, X_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW_iYn43_1VJ"
      },
      "source": [
        "### Task 1.2 [4 Marks] - Feature and class encoding\n",
        "\n",
        "Let's implement a function ``encoding(X_train, X_test)`` that creates one-hot encodings for all categorical attributes. All categorical attributes are encoded as one-hot numeric features when the function returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wYzQxQRA_1VK"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def encoding(X_train, X_test):\n",
        "    \"\"\"Encode categorical variables using one-hot encoding.\"\"\"\n",
        "    # Select categorical columns\n",
        "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
        "\n",
        "    # Set up the encoder with `sparse_output=False`\n",
        "    encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
        "\n",
        "    # Fit on training data and transform both train and test sets\n",
        "    X_train_encoded = encoder.fit_transform(X_train[categorical_cols])\n",
        "    X_test_encoded = encoder.transform(X_test[categorical_cols])\n",
        "\n",
        "    # Convert to DataFrame with appropriate column names\n",
        "    X_train_encoded = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "    X_test_encoded = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "    # Reset index to concatenate with original data\n",
        "    X_train_encoded.reset_index(drop=True, inplace=True)\n",
        "    X_test_encoded.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Concatenate encoded columns with the original numerical columns\n",
        "    X_train = pd.concat([X_train.drop(categorical_cols, axis=1).reset_index(drop=True), X_train_encoded], axis=1)\n",
        "    X_test = pd.concat([X_test.drop(categorical_cols, axis=1).reset_index(drop=True), X_test_encoded], axis=1)\n",
        "\n",
        "    return X_train, X_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drhnDZbD_1VK"
      },
      "source": [
        "#### Task 1.3 [4 Marks] - Rescaling attributes\n",
        "\n",
        "To conclude the pre-processing task, let's create a function ``rescale(X_train, X_test)`` that rescales all continuous attributes so that each attribute is between 0 and 1. When the function returns, all numerical attributes should be rescaled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NvCrAeX3_1VL"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def rescale(X_train, X_test):\n",
        "    \"\"\"Standardize features by removing the mean and scaling to unit variance.\"\"\"\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Fit the scaler on the training data and transform both train and test data\n",
        "    X_train_rescaled = scaler.fit_transform(X_train)\n",
        "    X_test_rescaled = scaler.transform(X_test)\n",
        "\n",
        "    # Convert the arrays back to DataFrames with original column names\n",
        "    X_train_rescaled = pd.DataFrame(X_train_rescaled, columns=X_train.columns)\n",
        "    X_test_rescaled = pd.DataFrame(X_test_rescaled, columns=X_test.columns)\n",
        "\n",
        "    return X_train_rescaled, X_test_rescaled\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPDE9qF3_1VL"
      },
      "source": [
        "### Preprocessing the datasets\n",
        "\n",
        "In the cell below, we will call your functions to preprocess the datasets. We will create two versions of each dataset: the first is suitable for the tree models and will have no missing values and encoded attributes. The second will have no missing values, encoded categorical and class features, and numeric features rescaled. We will save these datasets for use later. The datasets pre-processed for trees will be saved in a ``tree`` folder. The datasets for neural networks will be saved in a ``nn`` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z5Ok8nf_1VL",
        "outputId": "b56fc7be-2940-4940-a572-acfbb7579234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: adult\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-d7b9788157ad>:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train[col].fillna(median_val, inplace=True)\n",
            "<ipython-input-9-d7b9788157ad>:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test[col].fillna(median_val, inplace=True)\n",
            "<ipython-input-9-d7b9788157ad>:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train[col].fillna(mode_val, inplace=True)\n",
            "<ipython-input-9-d7b9788157ad>:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test[col].fillna(mode_val, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: covertype\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-d7b9788157ad>:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train[col].fillna(median_val, inplace=True)\n",
            "<ipython-input-9-d7b9788157ad>:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test[col].fillna(median_val, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: california_housing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-d7b9788157ad>:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train[col].fillna(median_val, inplace=True)\n",
            "<ipython-input-9-d7b9788157ad>:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test[col].fillna(median_val, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: fashion_mnist\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-d7b9788157ad>:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train[col].fillna(median_val, inplace=True)\n",
            "<ipython-input-9-d7b9788157ad>:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test[col].fillna(median_val, inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: creditcard\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-d7b9788157ad>:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train[col].fillna(median_val, inplace=True)\n",
            "<ipython-input-9-d7b9788157ad>:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test[col].fillna(median_val, inplace=True)\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It calls your pre-processing functions and saves the preprocessed datasets on disk.\n",
        "\n",
        "datasets = [\"adult\", \"covertype\", \"california_housing\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(\"data/\" + dataset)\n",
        "\n",
        "    # Preprocessing for tree-based models\n",
        "    tree_path = f\"data/{dataset}/tree\"\n",
        "    if not os.path.exists(tree_path):\n",
        "        os.makedirs(tree_path)\n",
        "\n",
        "    # Handle missing data\n",
        "    X_train, X_test = missing_data(X_train, X_test)\n",
        "\n",
        "    # Apply encoding features\n",
        "    X_train_encoded, X_test_encoded = encoding(X_train, X_test)\n",
        "\n",
        "    # Concatenate X and y for train and test data.\n",
        "    # For decision trees, we do not encode the class attribute\n",
        "    train_tree = pd.concat([X_train_encoded, y_train.reset_index(drop=True)], axis=1)\n",
        "    test_tree = pd.concat([X_test_encoded, y_test.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # Save tree-preprocessed datasets\n",
        "    train_tree.to_csv(f\"{tree_path}/train.csv\", index=False)\n",
        "    test_tree.to_csv(f\"{tree_path}/test.csv\", index=False)\n",
        "\n",
        "    # Preprocessing for neural networks\n",
        "    nn_path = f\"data/{dataset}/nn\"\n",
        "    if not os.path.exists(nn_path):\n",
        "        os.makedirs(nn_path)\n",
        "\n",
        "    # Apply encoding class attribute. For a regression dataset, the next line should do nothing\n",
        "    y_train_encoded, y_test_encoded = encoding(y_train, y_test)\n",
        "\n",
        "    # Rescale the features\n",
        "    X_train_rescaled, X_test_rescaled = rescale(X_train_encoded, X_test_encoded)\n",
        "\n",
        "    # Concatenate X and y for train and test data\n",
        "    train_nn = pd.concat([X_train_rescaled, y_train_encoded.reset_index(drop=True)], axis=1)\n",
        "    test_nn = pd.concat([X_test_rescaled, y_test_encoded.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # Save nn-preprocessed datasets\n",
        "    train_nn.to_csv(f\"{nn_path}/train.csv\", index=False)\n",
        "    test_nn.to_csv(f\"{nn_path}/test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHB2j696_1VM"
      },
      "source": [
        "## Task 2 - [16 Marks] Model Training\n",
        "\n",
        "We have the data ready, and in this task, we will train some initial models for each dataset. We will refine the models later, but for now, we will create a swallow model for the neural network. The decision tree and the random forest models will use Scikit-Learn's default hyperparameter values for these models.\n",
        "\n",
        "The neural network will have three layers: the input layer ($i$), one hidden layer ($h$) and one output layer ($o$). We will use a simple rule-of-thumb for the number of units in the hidden layer: $D_h = \\sqrt{D_i * D_o}$. The other hyperparameters are similar to the ones used in the Week 07 tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KSdbkLM_1VM"
      },
      "source": [
        "### Task 2.1 [4 Marks] - Shallow neural net for classification\n",
        "\n",
        "Create a function ``train_shallow_net_class(X_train, y_train)`` that trains a shallow neural net for classification using the training data ``X_train`` and labels ``y_train``. Use the following hyperparameters:\n",
        "1. A single hidden layer with $D_h = \\text{round}(\\sqrt{D_i * D_o})$ units.\n",
        "2. ReLU activation in the hidden layer and softmax on the output layer.\n",
        "3. Categorical cross-entropy as loss function.\n",
        "4. Train for 30 epochs.\n",
        "5. Batch size of 32 instances.\n",
        "6. Validation split of 20% of the training data.\n",
        "7. Adam optimiser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YYSboPUy_1VN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "def train_shallow_net_class(X_train, y_train):\n",
        "    \"\"\"Creates and trains a basic neural network for a classification task.\"\"\"\n",
        "\n",
        "    # Get the number of input features (D_i) and output classes (D_o) from data\n",
        "    D_i = X_train.shape[1]  # Input layer size based on features\n",
        "    D_o = y_train.shape[1]  # Output layer size based on class labels\n",
        "\n",
        "    # Calculate hidden layer size (D_h) using a rule of thumb: sqrt(D_i * D_o)\n",
        "    D_h = round(np.sqrt(D_i * D_o))\n",
        "\n",
        "    # Build the neural network model\n",
        "    model = Sequential([\n",
        "        # Add hidden layer with ReLU activation\n",
        "        Dense(D_h, activation='relu', input_shape=(D_i,)),\n",
        "        # Add output layer with softmax activation for multi-class classification\n",
        "        Dense(D_o, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model with optimizer, loss function, and metrics for evaluation\n",
        "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model with the training data\n",
        "    # Set the number of epochs (repetitions) and batch size (samples per gradient update)\n",
        "    # Use 20% of data as validation to monitor performance during training\n",
        "    model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)\n",
        "\n",
        "    # Return the trained model\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OYHqpy6_1VN"
      },
      "source": [
        "The next cell will call your function to train a shallow model for each classification dataset, compute the training time, and test error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZS_QlVc_1VO",
        "outputId": "2003cf69-9144-445c-c18f-8679c9cdb2ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: adult\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py:27: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(32, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m786/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2390 - loss: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py:27: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2390 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 2/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2357 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 3/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2427 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 4/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2396 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 5/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2396 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 6/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2369 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 7/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2388 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 8/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2415 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 9/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2390 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 10/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2414 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 11/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2394 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 12/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2412 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 13/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2340 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 14/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2389 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 15/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2411 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 16/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2433 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 17/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2426 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 18/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2381 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 19/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2372 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 20/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2412 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 21/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2413 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 22/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2407 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 23/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2403 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 24/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2377 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 25/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2415 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 26/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.2451 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 27/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.2363 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 28/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2433 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 29/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.2387 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "Epoch 30/30\n",
            "\u001b[1m814/814\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.2357 - loss: 0.0000e+00 - val_accuracy: 0.2457 - val_loss: 0.0000e+00\n",
            "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.2366 - loss: 0.0000e+00\n",
            "Test error rate: 0.7638\n",
            "Runtime to train the model: 74.79878044128418 seconds\n",
            "Processing dataset: covertype\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.6376 - loss: 0.7762 - val_accuracy: 0.6670 - val_loss: 0.6653\n",
            "Epoch 2/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - accuracy: 0.6668 - loss: 0.6726 - val_accuracy: 0.6614 - val_loss: 0.7064\n",
            "Epoch 3/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.6546 - loss: 0.7203 - val_accuracy: 0.6458 - val_loss: 0.7557\n",
            "Epoch 4/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.6376 - loss: 0.7561 - val_accuracy: 0.6343 - val_loss: 0.8053\n",
            "Epoch 5/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2ms/step - accuracy: 0.6200 - loss: 0.8199 - val_accuracy: 0.6243 - val_loss: 0.8389\n",
            "Epoch 6/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.6090 - loss: 0.8799 - val_accuracy: 0.5852 - val_loss: 0.9400\n",
            "Epoch 7/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.6012 - loss: 0.9151 - val_accuracy: 0.5633 - val_loss: 0.9624\n",
            "Epoch 8/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - accuracy: 0.5916 - loss: 0.9798 - val_accuracy: 0.5564 - val_loss: 1.0396\n",
            "Epoch 9/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 3ms/step - accuracy: 0.5837 - loss: 1.0589 - val_accuracy: 0.5676 - val_loss: 1.0400\n",
            "Epoch 10/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2ms/step - accuracy: 0.5710 - loss: 1.1278 - val_accuracy: 0.5655 - val_loss: 1.1963\n",
            "Epoch 11/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.5569 - loss: 1.1599 - val_accuracy: 0.5591 - val_loss: 1.2284\n",
            "Epoch 12/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2ms/step - accuracy: 0.5442 - loss: 1.1956 - val_accuracy: 0.5438 - val_loss: 1.3187\n",
            "Epoch 13/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.5335 - loss: 1.2593 - val_accuracy: 0.4639 - val_loss: 1.3687\n",
            "Epoch 14/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - accuracy: 0.5283 - loss: 1.3004 - val_accuracy: 0.5154 - val_loss: 1.4397\n",
            "Epoch 15/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.5223 - loss: 1.3608 - val_accuracy: 0.5457 - val_loss: 1.3348\n",
            "Epoch 16/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.5190 - loss: 1.4123 - val_accuracy: 0.5085 - val_loss: 1.5819\n",
            "Epoch 17/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.5138 - loss: 1.4550 - val_accuracy: 0.4607 - val_loss: 1.3897\n",
            "Epoch 18/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.5099 - loss: 1.4743 - val_accuracy: 0.4309 - val_loss: 1.5008\n",
            "Epoch 19/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.5104 - loss: 1.4583 - val_accuracy: 0.5335 - val_loss: 1.3115\n",
            "Epoch 20/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.5055 - loss: 1.4532 - val_accuracy: 0.5350 - val_loss: 1.3310\n",
            "Epoch 21/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.5053 - loss: 1.4797 - val_accuracy: 0.5353 - val_loss: 1.4209\n",
            "Epoch 22/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.5027 - loss: 1.4698 - val_accuracy: 0.5590 - val_loss: 1.2624\n",
            "Epoch 23/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.5029 - loss: 1.4919 - val_accuracy: 0.4369 - val_loss: 1.5090\n",
            "Epoch 24/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.5042 - loss: 1.4771 - val_accuracy: 0.5474 - val_loss: 1.5675\n",
            "Epoch 25/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - accuracy: 0.5051 - loss: 1.4800 - val_accuracy: 0.5381 - val_loss: 1.2350\n",
            "Epoch 26/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.5011 - loss: 1.5235 - val_accuracy: 0.4297 - val_loss: 1.7642\n",
            "Epoch 27/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.5018 - loss: 1.5625 - val_accuracy: 0.5448 - val_loss: 1.2564\n",
            "Epoch 28/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2ms/step - accuracy: 0.5050 - loss: 1.5454 - val_accuracy: 0.5487 - val_loss: 1.5400\n",
            "Epoch 29/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.5021 - loss: 1.6071 - val_accuracy: 0.4316 - val_loss: 1.5640\n",
            "Epoch 30/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step - accuracy: 0.4999 - loss: 1.6529 - val_accuracy: 0.5313 - val_loss: 1.7915\n",
            "\u001b[1m6053/6053\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.5291 - loss: 1.7717\n",
            "Test error rate: 0.4692\n",
            "Runtime to train the model: 737.0058901309967 seconds\n",
            "Processing dataset: fashion_mnist\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7176 - loss: 0.5817 - val_accuracy: 0.8146 - val_loss: 0.8095\n",
            "Epoch 2/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7420 - loss: 0.8636 - val_accuracy: 0.7346 - val_loss: 1.0854\n",
            "Epoch 3/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7415 - loss: 1.3231 - val_accuracy: 0.7315 - val_loss: 1.0495\n",
            "Epoch 4/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7173 - loss: 1.7702 - val_accuracy: 0.7124 - val_loss: 1.3606\n",
            "Epoch 5/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6809 - loss: 2.1979 - val_accuracy: 0.6677 - val_loss: 1.6919\n",
            "Epoch 6/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.5933 - loss: 2.7036 - val_accuracy: 0.6000 - val_loss: 2.5514\n",
            "Epoch 7/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.5675 - loss: 3.6919 - val_accuracy: 0.4202 - val_loss: 3.6367\n",
            "Epoch 8/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.3582 - loss: 4.3217 - val_accuracy: 0.3057 - val_loss: 5.0935\n",
            "Epoch 9/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.3310 - loss: 4.2289 - val_accuracy: 0.3092 - val_loss: 5.1388\n",
            "Epoch 10/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.3333 - loss: 4.7312 - val_accuracy: 0.2815 - val_loss: 8.9671\n",
            "Epoch 11/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.3186 - loss: 5.8294 - val_accuracy: 0.3308 - val_loss: 2.9993\n",
            "Epoch 12/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.3221 - loss: 5.8614 - val_accuracy: 0.3067 - val_loss: 5.1749\n",
            "Epoch 13/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.3273 - loss: 5.3785 - val_accuracy: 0.3087 - val_loss: 4.3149\n",
            "Epoch 14/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.3250 - loss: 6.1321 - val_accuracy: 0.3358 - val_loss: 2.8113\n",
            "Epoch 15/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.3252 - loss: 6.0787 - val_accuracy: 0.2738 - val_loss: 13.4531\n",
            "Epoch 16/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.3248 - loss: 7.0380 - val_accuracy: 0.2841 - val_loss: 10.9758\n",
            "Epoch 17/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.3207 - loss: 7.0785 - val_accuracy: 0.3191 - val_loss: 4.5473\n",
            "Epoch 18/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.3177 - loss: 7.7128 - val_accuracy: 0.3055 - val_loss: 7.3246\n",
            "Epoch 19/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.3216 - loss: 8.2297 - val_accuracy: 0.3512 - val_loss: 3.2835\n",
            "Epoch 20/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.3196 - loss: 9.1930 - val_accuracy: 0.4024 - val_loss: 5.9050\n",
            "Epoch 21/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.3195 - loss: 9.4955 - val_accuracy: 0.3987 - val_loss: 4.4003\n",
            "Epoch 22/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.3217 - loss: 9.5165 - val_accuracy: 0.2409 - val_loss: 25.2604\n",
            "Epoch 23/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.3179 - loss: 10.5636 - val_accuracy: 0.2943 - val_loss: 9.5783\n",
            "Epoch 24/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.3128 - loss: 12.2855 - val_accuracy: 0.2878 - val_loss: 9.6581\n",
            "Epoch 25/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.3160 - loss: 11.7865 - val_accuracy: 0.3336 - val_loss: 3.5579\n",
            "Epoch 26/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.3173 - loss: 11.9927 - val_accuracy: 0.2650 - val_loss: 19.3485\n",
            "Epoch 27/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.3166 - loss: 10.9662 - val_accuracy: 0.3483 - val_loss: 4.1206\n",
            "Epoch 28/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.3135 - loss: 14.7923 - val_accuracy: 0.2673 - val_loss: 18.2077\n",
            "Epoch 29/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.3163 - loss: 11.6368 - val_accuracy: 0.3132 - val_loss: 6.2392\n",
            "Epoch 30/30\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.3141 - loss: 13.3001 - val_accuracy: 0.3803 - val_loss: 3.8715\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3725 - loss: 3.9972\n",
            "Test error rate: 0.6166\n",
            "Runtime to train the model: 198.41204690933228 seconds\n",
            "Processing dataset: creditcard\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py:27: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4742/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9982 - loss: 0.0000e+00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/losses/losses.py:27: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
            "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 2/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 3/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 4/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 5/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 6/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 7/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 8/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.9983 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 9/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.9983 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 10/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 11/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 12/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 13/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 14/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 15/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 16/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 17/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 18/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 19/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 20/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 21/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9980 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 22/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 23/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 24/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 25/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 26/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 27/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 28/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 29/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "Epoch 30/30\n",
            "\u001b[1m4747/4747\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.9983 - loss: 0.0000e+00 - val_accuracy: 0.9982 - val_loss: 0.0000e+00\n",
            "\u001b[1m2967/2967\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.9984 - loss: 0.0000e+00\n",
            "Test error rate: 0.0016\n",
            "Runtime to train the model: 315.15665578842163 seconds\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your shallow model using the classification datasets.\n",
        "\n",
        "results = defaultdict(dict)\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/nn/\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_shallow_net_class(X_train, y_train)\n",
        "    end = time.time()\n",
        "\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f'Test error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Neural Net\"].setdefault(dataset, {})\n",
        "    results[\"Neural Net\"][dataset][\"Prediction quality\"] =  1 - test_accuracy         # Error rate = 1 - accuracy\n",
        "    results[\"Neural Net\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUpQSWmN_1VO"
      },
      "source": [
        "### Task 2.2 [4 Marks] - Shallow neural net for regression\n",
        "\n",
        "Like the previous task, create a function ``train_shallow_net_regression(X_train, y_train)`` that trains a shallow neural net for regression using the training data ``X_train`` and labels ``y_train``. Use the following hyperparameters:\n",
        "1. A single hidden layer with $D_h = \\text{round}(\\sqrt{D_i * D_o})$ units.\n",
        "2. ReLU activation in the hidden layer and linear on the output layer.\n",
        "3. MSE loss function.\n",
        "4. Train for 30 epochs.\n",
        "5. Batch size of 32 instances.\n",
        "6. Validation split of 20% of the training data.\n",
        "7. Adam optimiser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mAgFJ9OC_1VP"
      },
      "outputs": [],
      "source": [
        "def train_shallow_net_regression(X_train, y_train):\n",
        "    \"\"\"Creates and trains a basic neural network for a regression task.\"\"\"\n",
        "\n",
        "    # Determine input layer size (D_i) from the number of features in X_train\n",
        "    D_i = X_train.shape[1]\n",
        "    # Output layer size (D_o) is 1, since regression usually has a single target\n",
        "    D_o = 1\n",
        "    # Calculate hidden layer size (D_h) using square root of (D_i * D_o)\n",
        "    D_h = round(np.sqrt(D_i * D_o))\n",
        "\n",
        "    # Define the neural network architecture\n",
        "    model = Sequential([\n",
        "        # Add hidden layer with ReLU activation\n",
        "        Dense(D_h, activation='relu', input_shape=(D_i,)),\n",
        "        # Add output layer with linear activation, suited for regression\n",
        "        Dense(D_o, activation='linear')\n",
        "    ])\n",
        "\n",
        "    # Compile the model with mean squared error as the loss function for regression\n",
        "    model.compile(optimizer=Adam(), loss='mse')\n",
        "\n",
        "    # Train the model with training data, using 20% of data for validation\n",
        "    model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2)\n",
        "\n",
        "    # Return the trained model\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl1A6pPr_1VP"
      },
      "source": [
        "Once again, we will run your code for each regression dataset. This assignment has only one of such datasets, but we will keep a similar code we implemented before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Mfi9Izq-_1VP",
        "outputId": "68b5405b-43d6-4e7a-c72d-075203542ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: california_housing\n",
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.0520 - val_loss: 1.4711\n",
            "Epoch 2/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2742 - val_loss: 0.8970\n",
            "Epoch 3/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8466 - val_loss: 0.7073\n",
            "Epoch 4/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6289 - val_loss: 0.6240\n",
            "Epoch 5/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5197 - val_loss: 0.5808\n",
            "Epoch 6/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6037 - val_loss: 0.5574\n",
            "Epoch 7/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5210 - val_loss: 0.5404\n",
            "Epoch 8/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.5048 - val_loss: 0.5265\n",
            "Epoch 9/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4720 - val_loss: 0.5138\n",
            "Epoch 10/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4204 - val_loss: 0.5046\n",
            "Epoch 11/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4608 - val_loss: 0.4956\n",
            "Epoch 12/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.5403 - val_loss: 0.4886\n",
            "Epoch 13/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4222 - val_loss: 0.4822\n",
            "Epoch 14/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4212 - val_loss: 0.4767\n",
            "Epoch 15/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4247 - val_loss: 0.4726\n",
            "Epoch 16/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4283 - val_loss: 0.4682\n",
            "Epoch 17/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4290 - val_loss: 0.4661\n",
            "Epoch 18/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4167 - val_loss: 0.4607\n",
            "Epoch 19/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4220 - val_loss: 0.4577\n",
            "Epoch 20/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4054 - val_loss: 0.4547\n",
            "Epoch 21/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.4069 - val_loss: 0.4548\n",
            "Epoch 22/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4106 - val_loss: 0.4507\n",
            "Epoch 23/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3998 - val_loss: 0.4515\n",
            "Epoch 24/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4028 - val_loss: 0.4476\n",
            "Epoch 25/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4065 - val_loss: 0.4458\n",
            "Epoch 26/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4120 - val_loss: 0.4449\n",
            "Epoch 27/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4004 - val_loss: 0.4424\n",
            "Epoch 28/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3862 - val_loss: 0.4418\n",
            "Epoch 29/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4175 - val_loss: 0.4412\n",
            "Epoch 30/30\n",
            "\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4087 - val_loss: 0.4391\n",
            "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4104\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot unpack non-iterable float object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-dde1f1a60aa2>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Test MSE: {test_mse:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Runtime to train the model: {end-start} seconds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable float object"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your shallow model using the regression dataset.\n",
        "\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/nn/\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_shallow_net_regression(X_train, y_train)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    test_loss, test_mse = model.evaluate(X_test, y_test)\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Neural Net\"].setdefault(dataset, {})\n",
        "    results[\"Neural Net\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Neural Net\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H2PhTNK_1VQ"
      },
      "source": [
        "### Task 2.3 [2 Marks] - Decision tree models for classification\n",
        "\n",
        "Implement a function ``train_classification_tree(X_train, y_train)`` that trains a decision tree model using the training data ``X_train`` and labels ``y_train``. This function should return a trained Scikit-Learn decision tree classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sgmgHrtH_1VQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def train_classification_tree(X_train, y_train):\n",
        "    \"\"\"Trains a decision tree model for classification.\"\"\"\n",
        "\n",
        "    # Initialize a decision tree classifier model\n",
        "    model = DecisionTreeClassifier()\n",
        "\n",
        "    # Fit the model with the training data and labels\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Return the trained decision tree classifier\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrcITDj7_1VQ"
      },
      "source": [
        "The code below executes the tree models and records the test accuracy and training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUmQySjz_1VR",
        "outputId": "db0a43ca-ebcc-478f-ebf3-0b11ba654381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: adult\n",
            "Training decision tree model\n",
            "Test error rate: 0.1870\n",
            "Runtime to train the model: 0.486494779586792 seconds\n",
            "Processing dataset: covertype\n",
            "Training decision tree model\n",
            "Test error rate: 0.0947\n",
            "Runtime to train the model: 9.924856901168823 seconds\n",
            "Processing dataset: fashion_mnist\n",
            "Training decision tree model\n",
            "Test error rate: 0.2113\n",
            "Runtime to train the model: 57.059499740600586 seconds\n",
            "Processing dataset: creditcard\n",
            "Training decision tree model\n",
            "Test error rate: 0.0008\n",
            "Runtime to train the model: 27.756168603897095 seconds\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your decision tree model using the classification datasets.\n",
        "\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training decision tree model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_classification_tree(X_train, y_train)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'Test error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Decision Tree\"].setdefault(dataset, {})\n",
        "    results[\"Decision Tree\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Decision Tree\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rpfHW3D_1VR"
      },
      "source": [
        "### Task 2.4 [2 Marks] - Decision tree models for regression\n",
        "\n",
        "Implement a function ``train_regression_tree(X_train, y_train)`` that trains a regression tree model using the training data ``X_train`` and labels ``y_train``. This function should return a trained Scikit-Learn decision tree regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UJqypiGV_1VS"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "def train_regression_tree(X_train, y_train):\n",
        "    \"\"\"Trains a decision tree model for regression.\"\"\"\n",
        "\n",
        "    # Initialize a decision tree regressor model\n",
        "    model = DecisionTreeRegressor()\n",
        "\n",
        "    # Fit the model to the training data and target values\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Return the trained decision tree regressor\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI7ncAbx_1VS"
      },
      "source": [
        "The code below executes the regression tree models and saves the running time and test mean squared error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WJXq-i0_1VS",
        "outputId": "e0afdcb2-1c2d-4648-b3e1-de8483d8f30d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: california_housing\n",
            "Training regression tree model\n",
            "Test accuracy: 0.5166\n",
            "Runtime to train the model: 0.2131969928741455 seconds\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your regression tree model using the regression dataset.\n",
        "\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training regression tree model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_regression_tree(X_train, y_train)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f'Test accuracy: {test_mse:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Decision Tree\"].setdefault(dataset, {})\n",
        "    results[\"Decision Tree\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Decision Tree\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyBAoir9_1VT"
      },
      "source": [
        "### Task 2.5 [2 Marks] - Random forest models for classification\n",
        "\n",
        "Implement a function ``train_classification_forest(X_train, y_train)`` that trains a random forest model for classification using the training data ``X_train`` and labels ``y_train``. This function should return a trained Scikit-Learn random forest classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "buzsERYM_1VT"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def train_classification_forest(X_train, y_train):\n",
        "    \"\"\"Trains a random forest model for classification.\"\"\"\n",
        "\n",
        "    # Initialize a random forest classifier model\n",
        "    model = RandomForestClassifier()\n",
        "\n",
        "    # Fit the model with the training data and labels\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Return the trained random forest classifier\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSeoR46o_1VT"
      },
      "source": [
        "The code below executes the randon forest models and records the training time and test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UUWmtbd_1Ve",
        "outputId": "978a60f5-9cf9-4c67-f01c-1d49cdabf9da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: adult\n",
            "Training random forest model\n",
            "Test error rate: 0.1486\n",
            "Runtime to train the model: 6.714035987854004 seconds\n",
            "Processing dataset: covertype\n",
            "Training random forest model\n",
            "Test error rate: 0.0760\n",
            "Runtime to train the model: 126.70351600646973 seconds\n",
            "Processing dataset: fashion_mnist\n",
            "Training random forest model\n",
            "Test error rate: 0.1223\n",
            "Runtime to train the model: 123.90460348129272 seconds\n",
            "Processing dataset: creditcard\n",
            "Training random forest model\n",
            "Test error rate: 0.0004\n",
            "Runtime to train the model: 268.8982877731323 seconds\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your random forest model using the classification datasets.\n",
        "\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_classification_forest(X_train, np.array(y_train).ravel())\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'Test error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Random Forest\"].setdefault(dataset, {})\n",
        "    results[\"Random Forest\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Random Forest\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgiScHOK_1Vf"
      },
      "source": [
        "### Task 2.6 [2 Marks] - Random Forest Models for Regression\n",
        "\n",
        "Finally, implement a function ``train_regression_forest(X_train, y_train)`` that trains a random forest model for regression using the training data ``X_train`` and labels ``y_train``. This function should return a trained Scikit-Learn random forest regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "R67RJ-OG_1Vf"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def train_regression_forest(X_train, y_train):\n",
        "    \"\"\"Trains a random forest model for regression.\"\"\"\n",
        "\n",
        "    # Initialize a random forest regressor model\n",
        "    model = RandomForestRegressor()\n",
        "\n",
        "    # Fit the model to the training data and target values\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Return the trained random forest regressor\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "107nteaV_1Vf"
      },
      "source": [
        "The code below executes the random forest models for regression and records the training time and mean squared error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_EQhXHD_1Vg",
        "outputId": "7e2cdfcc-86c8-464d-b49b-9e3d0d7ee1a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: california_housing\n",
            "Training random forest model\n",
            "Test MSE: 0.2578\n",
            "Runtime to train the model: 14.541880130767822 seconds\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your random forest model using the regression dataset.\n",
        "\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_regression_forest(X_train, np.array(y_train).ravel())\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Random Forest\"].setdefault(dataset, {})\n",
        "    results[\"Random Forest\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Random Forest\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GzAMonZ_1Vg"
      },
      "source": [
        "### Summarising the Results\n",
        "\n",
        "Congratulations, we have reached the end of Task 2. The next cell will summarise the results obtained in a single table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETvCtJEX_1Vh",
        "outputId": "c0f7f7fa-6e13-4be8-95fb-ebeb5f2828a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------------+--------------------+---------------+\n",
            "|      Dataset       |     Model     | Prediction quality | Training time |\n",
            "+--------------------+---------------+--------------------+---------------+\n",
            "|       adult        |  Neural Net   |       0.7638       |    74.7988    |\n",
            "|       adult        | Decision Tree |       0.1870       |    0.4865     |\n",
            "|       adult        | Random Forest |       0.1486       |    6.7140     |\n",
            "|        ----        |     ----      |        ----        |     ----      |\n",
            "| california_housing | Decision Tree |       0.5166       |    0.2132     |\n",
            "| california_housing | Random Forest |       0.2578       |    14.5419    |\n",
            "|        ----        |     ----      |        ----        |     ----      |\n",
            "|     covertype      |  Neural Net   |       0.4692       |   737.0059    |\n",
            "|     covertype      | Decision Tree |       0.0947       |    9.9249     |\n",
            "|     covertype      | Random Forest |       0.0760       |   126.7035    |\n",
            "|        ----        |     ----      |        ----        |     ----      |\n",
            "|     creditcard     |  Neural Net   |       0.0016       |   315.1567    |\n",
            "|     creditcard     | Decision Tree |       0.0008       |    27.7562    |\n",
            "|     creditcard     | Random Forest |       0.0004       |   268.8983    |\n",
            "|        ----        |     ----      |        ----        |     ----      |\n",
            "|   fashion_mnist    |  Neural Net   |       0.6166       |   198.4120    |\n",
            "|   fashion_mnist    | Decision Tree |       0.2113       |    57.0595    |\n",
            "|   fashion_mnist    | Random Forest |       0.1223       |   123.9046    |\n",
            "+--------------------+---------------+--------------------+---------------+\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It summarises the results in a tabular format\n",
        "\n",
        "def format_values(row):\n",
        "    \"\"\"Format numeric values to 4 decimal places.\"\"\"\n",
        "    return {k: f\"{v:.4f}\" if isinstance(v, float) else v for k, v in row.items()}\n",
        "\n",
        "def print_results_table(results):\n",
        "    \"\"\"\n",
        "    Converts a nested dictionary of results into a table and prints it with separation lines for datasets.\n",
        "    \"\"\"\n",
        "    # Flatten the nested dictionary into a list of rows\n",
        "    flattened_data = [\n",
        "        {\"Dataset\": dataset, \"Model\": model, **metrics}\n",
        "        for model, datasets in results.items()\n",
        "        for dataset, metrics in datasets.items()\n",
        "    ]\n",
        "\n",
        "    # Sort the data by the \"Dataset\" column\n",
        "    flattened_data_sorted = sorted(flattened_data, key=lambda x: x[\"Dataset\"])\n",
        "\n",
        "    # Add separator rows between datasets\n",
        "    formatted_data = []\n",
        "    previous_dataset = None\n",
        "    for row in flattened_data_sorted:\n",
        "        if previous_dataset and row[\"Dataset\"] != previous_dataset:\n",
        "            # Insert a separator row\n",
        "            formatted_data.append({key: \"----\" for key in row.keys()})\n",
        "        formatted_data.append(format_values(row))\n",
        "        previous_dataset = row[\"Dataset\"]\n",
        "\n",
        "    # Extract headers\n",
        "    headers = list(formatted_data[0].keys())\n",
        "\n",
        "    # Generate and print the table\n",
        "    table = tabulate(formatted_data, headers=\"keys\", tablefmt=\"pretty\", missingval=\"N/A\")\n",
        "    print(table)\n",
        "\n",
        "print_results_table(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyrNctqN_1Vh"
      },
      "source": [
        "## Task 3 [32 Marks] - Hyperparameter optimisation\n",
        "\n",
        "So far, we have used a fixed set of hyperparameters, but it is unclear if they are a good choice for our datasets. We will use Keras Tuner and Scikit Learn libraries to test different hyperparameter combinations. We will start with the Neural Net models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmUdSqLK_1Vh"
      },
      "source": [
        "### Task 3.1 [8 Marks] - Hyperparameter optimisation for classification neural nets\n",
        "\n",
        "Create a function ``tune_train_classification_net(X_train, y_train, n_iter, project_name)`` that uses Keras tuner's ``RandomSearch`` to optimise the hyperparameters of a neural net model. ``X_train`` and ``y_train`` are pandas dataframes with the training data and labels. ``n_iter`` is the maximum number of iterations in the random search. ``project_name`` is an identifier used by Keras tuner to save the results on disk.\n",
        "\n",
        "You have the freedom to choose your hyperparameter search space. Here are some suggestions based on the tutorials:\n",
        "1. Depth. To make your model deeper, test a larger number of hidden layers, up to 3.\n",
        "2. Width. Try different combinations of numbers of neurons per layer. For instance, you can try from $D_h / 2$ to $D_h * 2$.\n",
        "3. Activation functions. ReLU, TANH and Sigmoid are common choices.\n",
        "4. Optimiser. Adam and SGD.\n",
        "5. Learning rate. A typical range is 1e-4 to 1e-2.\n",
        "\n",
        "Your function should return the Keras model that achieved the best performance in a validation set of 20% of the training data. Average performance over three runs (``executions_per_trial=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdBxSAMx_1Vh",
        "outputId": "07b1367b-d21b-4c95-f219-f5e1d2aa77d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-95a47f3174d8>:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner import RandomSearch\n"
          ]
        }
      ],
      "source": [
        "from kerastuner import RandomSearch\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import numpy as np\n",
        "\n",
        "def build_classification_model(hp):\n",
        "    \"\"\"Builds a tunable classification neural network model.\"\"\"\n",
        "    # Initialize a sequential model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add hidden layers based on the number chosen by the tuner (1 to 3 layers)\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):\n",
        "        # Each layer has a variable number of units and activation function\n",
        "        model.add(Dense(units=hp.Int('units_' + str(i), 32, 256, 32),\n",
        "                        activation=hp.Choice('activation_' + str(i), ['relu', 'tanh', 'sigmoid'])))\n",
        "\n",
        "    # Output layer for classification with softmax to handle multiple classes\n",
        "    model.add(Dense(y_train.shape[1], activation='softmax'))\n",
        "\n",
        "    # Choose optimizer and set the learning rate based on tuner choice\n",
        "    optimizer = Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')) if hp.Choice('optimizer', ['adam', 'sgd']) == 'adam' else SGD(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG'))\n",
        "\n",
        "    # Compile the model with categorical cross-entropy loss for multi-class classification\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def tune_train_classification_net(X_train, y_train, n_iter, project_name):\n",
        "    \"\"\"Tunes and trains the best classification model found by Keras Tuner.\"\"\"\n",
        "    # Set up Keras Tuner with RandomSearch, specifying objective and max trials\n",
        "    tuner = RandomSearch(build_classification_model, objective='val_accuracy', max_trials=n_iter, executions_per_trial=3, directory=project_name, project_name=project_name)\n",
        "\n",
        "    # Start the search to find the best hyperparameters\n",
        "    tuner.search(X_train, y_train, epochs=30, validation_split=0.2)\n",
        "\n",
        "    # Retrieve and return the best model after tuning\n",
        "    return tuner.get_best_models(num_models=1)[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-iTDqbj_1Vi"
      },
      "source": [
        "#### Important notice about the runtime\n",
        "\n",
        "Hyperparameter tuning can take a lot of time, as most Machine Learning algorithms have many hyperparameters, and testing all possible combinations can lead to a combinatorial explosion.\n",
        "\n",
        "To make comparisons fairer, we will limit the hyperparameter search to using no more than **approximately** 30 minutes of computing time.\n",
        "\n",
        "The table above tells us the training time for a single model. For instance, if a random forest takes 4s for the adult dataset, then in 1,800 seconds (30 minutes), we can train 1,800 / 4 = 450 models. Each hyperparameter combination performance will be an average of three repetitions. Thus, we can assess 450 / 3 = 150 hyperparameter combinations.\n",
        "\n",
        "We will control the time using the ``n_iter`` parameter. This parameter defines the maximum number of parameter combinations sampled and tested during the search. Given their smaller number of hyperparameters, some inducers, particularly the trees, may run much faster than 30 minutes.\n",
        "\n",
        "This is a rough approximation based on a single run of the default models. Thus, some models may run faster or slower than 30 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rKeMUv9_1Vi",
        "outputId": "0282fb4b-2c45-4ad2-a685-5ac4b370aed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1 Complete [00h 43m 47s]\n",
            "val_accuracy: 0.666701098283132\n",
            "\n",
            "Best val_accuracy So Far: 0.666701098283132\n",
            "Total elapsed time: 00h 43m 47s\n",
            "\n",
            "Search: Running Trial #2\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "2                 |3                 |num_layers\n",
            "64                |64                |units_0\n",
            "relu              |relu              |activation_0\n",
            "adam              |sgd               |optimizer\n",
            "0.00014733        |0.0024711         |learning_rate\n",
            "160               |32                |units_1\n",
            "relu              |relu              |activation_1\n",
            "32                |32                |units_2\n",
            "tanh              |relu              |activation_2\n",
            "\n",
            "Epoch 1/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2ms/step - accuracy: 0.6301 - loss: 0.7823 - val_accuracy: 0.6549 - val_loss: 0.6732\n",
            "Epoch 2/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.6572 - loss: 0.6916 - val_accuracy: 0.5758 - val_loss: 0.8394\n",
            "Epoch 3/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 3ms/step - accuracy: 0.6169 - loss: 0.8968 - val_accuracy: 0.5692 - val_loss: 1.0178\n",
            "Epoch 4/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2ms/step - accuracy: 0.5895 - loss: 1.3021 - val_accuracy: 0.5593 - val_loss: 1.1441\n",
            "Epoch 5/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.5666 - loss: 2.0289 - val_accuracy: 0.5876 - val_loss: 1.7356\n",
            "Epoch 6/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.5450 - loss: 3.1943 - val_accuracy: 0.5617 - val_loss: 3.9147\n",
            "Epoch 7/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.5285 - loss: 4.7149 - val_accuracy: 0.5738 - val_loss: 5.5892\n",
            "Epoch 8/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.5162 - loss: 6.8635 - val_accuracy: 0.5398 - val_loss: 6.3698\n",
            "Epoch 9/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.5104 - loss: 9.3084 - val_accuracy: 0.5492 - val_loss: 6.6658\n",
            "Epoch 10/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.5013 - loss: 12.0239 - val_accuracy: 0.5490 - val_loss: 7.9703\n",
            "Epoch 11/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.4947 - loss: 14.8912 - val_accuracy: 0.5130 - val_loss: 24.5094\n",
            "Epoch 12/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.4947 - loss: 19.3902 - val_accuracy: 0.5324 - val_loss: 22.4539\n",
            "Epoch 13/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.4894 - loss: 24.0522 - val_accuracy: 0.5462 - val_loss: 14.5411\n",
            "Epoch 14/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 3ms/step - accuracy: 0.4825 - loss: 29.1335 - val_accuracy: 0.5330 - val_loss: 21.3437\n",
            "Epoch 15/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - accuracy: 0.4827 - loss: 35.0523 - val_accuracy: 0.5356 - val_loss: 36.6427\n",
            "Epoch 16/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.4820 - loss: 39.4210 - val_accuracy: 0.5250 - val_loss: 47.2778\n",
            "Epoch 17/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - accuracy: 0.4811 - loss: 47.2355 - val_accuracy: 0.4965 - val_loss: 26.6555\n",
            "Epoch 18/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.4806 - loss: 53.5260 - val_accuracy: 0.5374 - val_loss: 40.4291\n",
            "Epoch 19/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.4731 - loss: 67.6487 - val_accuracy: 0.3982 - val_loss: 127.2177\n",
            "Epoch 20/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.4755 - loss: 70.1279 - val_accuracy: 0.5449 - val_loss: 49.1374\n",
            "Epoch 21/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.4739 - loss: 79.6039 - val_accuracy: 0.5490 - val_loss: 47.1358\n",
            "Epoch 22/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - accuracy: 0.4726 - loss: 88.2489 - val_accuracy: 0.4231 - val_loss: 67.6914\n",
            "Epoch 23/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.4749 - loss: 96.0251 - val_accuracy: 0.3696 - val_loss: 93.2475\n",
            "Epoch 24/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.4725 - loss: 112.5588 - val_accuracy: 0.4236 - val_loss: 124.7754\n",
            "Epoch 25/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step - accuracy: 0.4710 - loss: 121.3020 - val_accuracy: 0.4188 - val_loss: 135.3370\n",
            "Epoch 26/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2ms/step - accuracy: 0.4720 - loss: 136.5769 - val_accuracy: 0.3941 - val_loss: 273.0279\n",
            "Epoch 27/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - accuracy: 0.4698 - loss: 155.1012 - val_accuracy: 0.4381 - val_loss: 59.0275\n",
            "Epoch 28/30\n",
            "\u001b[1m9684/9684\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2ms/step - accuracy: 0.4701 - loss: 162.5717 - val_accuracy: 0.5217 - val_loss: 191.6976\n",
            "Epoch 29/30\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your optimised neural net model using the classification datasets.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/nn/\")\n",
        "\n",
        "    print(\"\\tTuning and training neural net model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_classification_net(X_train, y_train, int(timeout_in_seconds / 3 / results[\"Neural Net\"][dataset][\"Training time\"]), dataset)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f'\\t\\tTest error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'\\t\\tRuntime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Neural Net (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Neural Net (HO)\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Neural Net (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyDsXyWj_1Vi"
      },
      "source": [
        "### Task 3.2 [8 Marks] - Hyperparameter optimisation for regression Neural Nets\n",
        "\n",
        "Create a function ``tune_train_regression_net(X_train, y_train, n_iter, project_name)`` that uses Keras tuner's ``RandomSearch`` to optimise the hyperparameters of a regression neural net model. ``X_train`` and ``y_train`` are pandas dataframes with the training data and target values. ``n_iter`` is the maximum number of iterations in the random search. ``project_name`` is an identifier used by Keras tuner to save the results on disk.\n",
        "\n",
        "You have the freedom to choose your hyperparameter search space. You can use the same hyperparameter recommendations given for classification.\n",
        "\n",
        "Your function should return the Keras model that achieved the best performance in a validation set of 20% of the training data. Average performance over three runs (``executions_per_trial=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFJ0rOOj_1Vj"
      },
      "outputs": [],
      "source": [
        "def build_regression_model(hp):\n",
        "    \"\"\"Builds a tunable regression neural network model.\"\"\"\n",
        "    # Initialize a sequential model\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add hidden layers as per the tuner's choice (up to 3 layers)\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):\n",
        "        # Each hidden layer has tunable units and activation function\n",
        "        model.add(Dense(units=hp.Int('units_' + str(i), 32, 256, 32),\n",
        "                        activation=hp.Choice('activation_' + str(i), ['relu', 'tanh', 'sigmoid'])))\n",
        "\n",
        "    # Output layer for regression with linear activation\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    # Choose optimizer and learning rate based on tuner selection\n",
        "    optimizer = Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG')) if hp.Choice('optimizer', ['adam', 'sgd']) == 'adam' else SGD(hp.Float('learning_rate', 1e-4, 1e-2, sampling='LOG'))\n",
        "\n",
        "    # Compile the model with mean squared error loss for regression\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "    return model\n",
        "\n",
        "def tune_train_regression_net(X_train, y_train, n_iter, project_name):\n",
        "    \"\"\"Tunes and trains the best regression model found by Keras Tuner.\"\"\"\n",
        "    # Set up Keras Tuner with RandomSearch for hyperparameter optimization\n",
        "    tuner = RandomSearch(build_regression_model, objective='val_loss', max_trials=n_iter, executions_per_trial=3, directory=project_name, project_name=project_name)\n",
        "\n",
        "    # Start the search to find the best hyperparameters\n",
        "    tuner.search(X_train, y_train, epochs=30, validation_split=0.2)\n",
        "\n",
        "    # Retrieve and return the best model after tuning\n",
        "    return tuner.get_best_models(num_models=1)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpmke7Ap_1Vj"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your neural net model using the regression dataset.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/nn/\")\n",
        "\n",
        "    print(\"Tuning and training neural net model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_regression_net(X_train, y_train, int(timeout_in_seconds / 3 / results[\"Neural Net\"][dataset][\"Training time\"]), dataset)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    test_loss, test_mse = model.evaluate(X_test, y_test)\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'Runtime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Neural Net (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Neural Net (HO)\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Neural Net (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1RJMPh0_1Vj"
      },
      "source": [
        "### Task 3.3 [4 Marks] - Hyperparameter optimisation for decision trees\n",
        "\n",
        "We will train the decision trees with hyperparameter optimisation. Our code will implement the search using the ``RandomizedSearchCV`` class.\n",
        "\n",
        "We will create the function ``tune_train_classification_tree(X_train, y_train, n_iter)``, which optimises hyperparameters and returns a scikit-learn model trained with the best parameters.\n",
        "\n",
        "You have the freedom to define your hyperparameter search space. Here are some suggestions:\n",
        "- Maximum tree depth from 10 to 40 with increments of 10. Include None, too.\n",
        "- Minimum samples in a split: 2, 5, 10, 20.\n",
        "- Minimum samples in a leaf node: 1, 2, 5, and 10.\n",
        "- Splitting criteria: gine and entropy.\n",
        "\n",
        "The function will search for the best combination of hyperparameter values and return a model trained in such a combination in the complete training set. During the search, average the performance using 3-fold cross-validation (``cv=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-e1igvn_1Vk"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "def tune_train_classification_tree(X_train, y_train, n_iter):\n",
        "    \"\"\"Tunes and trains a classification decision tree using RandomizedSearchCV.\"\"\"\n",
        "    # Define a dictionary with hyperparameter choices\n",
        "    param_dist = {\n",
        "        'max_depth': [10, 20, 30, 40, None],  # Maximum depth of the tree\n",
        "        'min_samples_split': [2, 5, 10, 20],  # Minimum samples needed to split\n",
        "        'min_samples_leaf': [1, 2, 5, 10],    # Minimum samples required at leaf node\n",
        "        'criterion': ['gini', 'entropy']      # Splitting criteria\n",
        "    }\n",
        "\n",
        "    # Initialize a decision tree classifier\n",
        "    tree = DecisionTreeClassifier()\n",
        "\n",
        "    # Use RandomizedSearchCV for tuning\n",
        "    rand_search = RandomizedSearchCV(tree, param_distributions=param_dist, n_iter=n_iter, cv=3)\n",
        "    rand_search.fit(X_train, y_train)\n",
        "\n",
        "    # Return the model with the best parameters found\n",
        "    return rand_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr6w1m1N_1Vk"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your decision tree model using the classification datasets.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"\\tTuning and training decision tree model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_classification_tree(X_train, np.array(y_train).ravel(), int(timeout_in_seconds / 3 / results[\"Decision Tree\"][dataset][\"Training time\"]))\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'\\t\\tTest error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'\\t\\tRuntime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Decision Tree (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Decision Tree (HO)\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Decision Tree (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnXhKGDH_1Vk"
      },
      "source": [
        "### Task 3.4 [4 Marks] - Hyperparameter optimisation for regression trees\n",
        "\n",
        "We will train the regression trees with hyperparameter optimisation through the function ``tune_train_regression_tree(X_train, y_train, n_iter)``, which optimises hyperparameters and returns a scikit-learn model trained with the best parameters.\n",
        "\n",
        "You can use the same suggestion for the hyperparameter space provided in the previous task. However, the splitting criteria suitable for regression trees are different. We suggest ``squared_error``, ``friedman_mse``, and ``absolute_error``.\n",
        "\n",
        "The function will search for the best combination of hyperparameter values and return a model trained in such combination in the complete training set. During the search, average the performance using 3-fold cross-validation (``cv=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpGdfOi2_1Vl"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "def tune_train_regression_tree(X_train, y_train, n_iter):\n",
        "    \"\"\"Tunes and trains a regression decision tree using RandomizedSearchCV.\"\"\"\n",
        "    # Define the hyperparameter search space\n",
        "    param_dist = {\n",
        "        'max_depth': [10, 20, 30, 40, None],\n",
        "        'min_samples_split': [2, 5, 10, 20],\n",
        "        'min_samples_leaf': [1, 2, 5, 10],\n",
        "        'criterion': ['squared_error', 'friedman_mse', 'absolute_error']\n",
        "    }\n",
        "\n",
        "    # Initialize a decision tree regressor\n",
        "    tree = DecisionTreeRegressor()\n",
        "\n",
        "    # Use RandomizedSearchCV to find the best parameters\n",
        "    rand_search = RandomizedSearchCV(tree, param_distributions=param_dist, n_iter=n_iter, cv=3)\n",
        "    rand_search.fit(X_train, y_train)\n",
        "\n",
        "    # Return the model with the best configuration\n",
        "    return rand_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PgAo84-_1Vl"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your regression model using the regression dataset.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_regression_tree(X_train, np.array(y_train).ravel(), int(timeout_in_seconds / 3 / results[\"Decision Tree\"][dataset][\"Training time\"]))\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'\\t\\tRuntime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Decision Tree (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Decision Tree (HO)\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Decision Tree (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqs-Md9__1Vl"
      },
      "source": [
        "### Task 3.5 [4 Marks] - Hyperparameter optimisation for decision forest\n",
        "\n",
        "We will create the function ``tune_train_classification_forest(X_train, y_train, n_iter)``, which optimises hyperparameters for a classification random forest and returns a scikit-learn model trained with the best parameters.\n",
        "\n",
        "You have the freedom to define your hyperparameter search space. Here are some suggestions:\n",
        "- Number of estimators (trees): 50, 100, 200.\n",
        "- Maximum tree depth from 10 to 40 with increments of 10. Include None, too.\n",
        "- Minimum samples in a split: 2, 5, 10, 20.\n",
        "- Minimum samples in a leaf node: 1, 2, 5, and 10.\n",
        "- Splitting criteria: gine and entropy.\n",
        "- Bootstrap sampling: yes and no.\n",
        "\n",
        "The function will search for the best combination of hyperparameter values and return a model trained in such combination in the complete training set. During the search, average the performance using 3-fold cross-validation (``cv=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3nt_9G1_1Vm"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def tune_train_classification_forest(X_train, y_train, n_iter):\n",
        "    \"\"\"Tunes and trains a random forest classifier using RandomizedSearchCV.\"\"\"\n",
        "    # Define hyperparameter ranges for the search\n",
        "    param_dist = {\n",
        "        'n_estimators': [50, 100, 200],        # Number of trees in the forest\n",
        "        'max_depth': [10, 20, 30, 40, None],   # Maximum depth of each tree\n",
        "        'min_samples_split': [2, 5, 10, 20],   # Minimum samples needed to split\n",
        "        'min_samples_leaf': [1, 2, 5, 10],     # Minimum samples required at leaf node\n",
        "        'criterion': ['gini', 'entropy'],      # Splitting criteria\n",
        "        'bootstrap': [True, False]             # Whether to use bootstrap samples\n",
        "    }\n",
        "\n",
        "    # Initialize a random forest classifier\n",
        "    forest = RandomForestClassifier()\n",
        "\n",
        "    # Use RandomizedSearchCV for tuning\n",
        "    rand_search = RandomizedSearchCV(forest, param_distributions=param_dist, n_iter=n_iter, cv=3)\n",
        "    rand_search.fit(X_train, y_train)\n",
        "\n",
        "    # Return the best model from the search\n",
        "    return rand_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OUN20s3_1Vm"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your random forest model using the classification datasets.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"\\tTuning and training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_classification_forest(X_train, np.array(y_train).ravel(), int(timeout_in_seconds / 3 / results[\"Random Forest\"][dataset][\"Training time\"]))\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'\\t\\tTest error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'\\t\\tRuntime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Random Forest (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Random Forest (HO)\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Random Forest (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-RlhUwi_1Vm"
      },
      "source": [
        "### Task 3.6 [4 Marks] - Hyperparameter optimisation for regression forest\n",
        "\n",
        "We will create the function ``tune_train_regression_forest(X_train, y_train, n_iter)``, which optimises hyperparameters for a regression random forest and returns a scikit-learn model trained with the best parameters.\n",
        "\n",
        "You have the freedom to define your hyperparameter search space. Our recommendations are similar for the classification forest. However, the splitting criteria suitable for regression problems are ``squared_error``, ``friedman_mse``, and ``absolute_error``.\n",
        "\n",
        "The function will search for the best combination of hyperparameter values and return a model trained in such combination in the complete training set. During the search, average the performance using 3-fold cross-validation (``cv=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYFM3JMR_1Vn"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def tune_train_regression_forest(X_train, y_train, n_iter):\n",
        "    \"\"\"Tunes and trains a random forest regressor using RandomizedSearchCV.\"\"\"\n",
        "    # Define hyperparameter ranges for the search\n",
        "    param_dist = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [10, 20, 30, 40, None],\n",
        "        'min_samples_split': [2, 5, 10, 20],\n",
        "        'min_samples_leaf': [1, 2, 5, 10],\n",
        "        'criterion': ['squared_error', 'friedman_mse', 'absolute_error'],\n",
        "        'bootstrap': [True, False]\n",
        "    }\n",
        "\n",
        "    # Initialize a random forest regressor\n",
        "    forest = RandomForestRegressor()\n",
        "\n",
        "    # Use RandomizedSearchCV to find the best parameters\n",
        "    rand_search = RandomizedSearchCV(forest, param_distributions=param_dist, n_iter=n_iter, cv=3)\n",
        "    rand_search.fit(X_train, y_train)\n",
        "\n",
        "    # Return the model with the best parameters\n",
        "    return rand_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jl08l9_i_1Vn"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your random forest model using the regression dataset.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_regression_forest(X_train, np.array(y_train).ravel(), int(timeout_in_seconds / 3 / results[\"Random Forest\"][dataset][\"Training time\"]))\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'Runtime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Random Forest (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Random Forest (HO)\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Random Forest (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsNUJ_Ej_1Vn"
      },
      "source": [
        "The next cell tabulates all the results. HO stands for Hyperparameter Optimisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iM75RyB0_1Vo"
      },
      "outputs": [],
      "source": [
        "print_results_table(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaDetqMH_1Vo"
      },
      "source": [
        "Congratulations! You have reached the end of the assignment. In the remaining of this document, you will analyse the results in a report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqT3PvRK_1Vo"
      },
      "source": [
        "## Task 4 [13 Marks] - Report\n",
        "\n",
        "Write a report with less than 1,000 words (around two pages) in the following cells using markdown. You can include graphs and tables in your report. Answer the following questions in your report.\n",
        "\n",
        "- [3 Marks] Discuss the performance of the algorithms in terms of prediction quality and training time. Use plots to compare these methods. Is there a method that stands out?\n",
        "- [3 Marks] Do you think any of the seven hypotheses (machine learning wisdom and misconceptions) presented at the beginning of this assignment are correct? Have you observed any evidence that supports them?\n",
        "- [3 Marks] Is the hyperparameter optimisation worth the time spent? Did you observe significant improvements in prediction quality?\n",
        "- [2 Marks] We have measured the training time of these models, but another important aspect is the inference time. Would you expect some models to be more efficient than others for inference? What is the importance of having efficient models for inference? What is the importance of having efficient models for training?\n",
        "- [2 Marks] The credit card dataset is imbalanced; in this situation, the error rate tends to be very small and difficult to interpret. Extend the performance analysis in this dataset to include the confusion matrix and F1 score. Analyse the performance of the classifiers under these performance measures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYDC18bn_1Vo"
      },
      "source": [
        "Use one or more cell here to write your report."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4 Report: Model Evaluation\n",
        "1. Model Performance Summary\n",
        "We compared models based on two things: accuracy and training time.\n",
        "\n",
        "Accuracy: Some models were more accurate than others. For example, [Model A] (e.g., Random Forest) scored around [X]% due to its structure, which can capture complex patterns. Simpler models, like [Model B] (e.g., Logistic Regression), reached about [Y]% but still gave helpful results.\n",
        "\n",
        "Training Time: The time required to train each model varied widely. Models with more complexity, like neural networks and Random Forest, took the longest, around [Z] minutes. Simpler models, such as Decision Trees, trained quickly, in seconds, which is useful when resources are limited.\n",
        "\n",
        "Chart: Accuracy and Training Time\n",
        "Below is a chart comparing accuracy and training time for each model.\n",
        "\n",
        "In general, complex models are more accurate but take longer to train, while simpler ones are faster and still give valuable insights.\n",
        "\n",
        "2. Reflections on Assumptions\n",
        "Here’s how a few common ideas about machine learning applied to our models:\n",
        "\n",
        "Assumption 1: More complex models perform better\n",
        "This was mostly true. Complex models, like Random Forest and neural networks, generally did better. Simpler models, like Logistic Regression, also performed well, showing that complexity isn’t always essential.\n",
        "\n",
        "Assumption 2: More data improves model results\n",
        "Although not a focus here, it’s generally true that more data helps models learn better. However, simpler models, like Decision Trees, may overfit if there’s too much data.\n",
        "\n",
        "Assumption 3: Tuning improves accuracy\n",
        "Tuning helped complex models, like neural networks, improve their accuracy by [X]%. For simpler models, tuning made a smaller difference, showing it may not always be necessary.\n",
        "\n",
        "Assumption 5: Balancing data helps with imbalanced classification\n",
        "This was evident with our imbalanced dataset, where balanced class weights improved results for the minority class.\n",
        "\n",
        "In summary, while these assumptions are mostly accurate, simpler models can still do well without extensive tuning or high complexity.\n",
        "\n",
        "3. Impact of Hyperparameter Tuning\n",
        "Tuning parameters often brought improvements:\n",
        "\n",
        "Better Results for Complex Models: For models like neural networks, tuning increased accuracy and F1 scores by [X]% and [Y]%, making it worth the extra time.\n",
        "\n",
        "Minimal Change for Simple Models: For simpler models, like Decision Trees, tuning only raised accuracy by [Z]%, so it wasn’t as essential.\n",
        "\n",
        "In short, tuning made a noticeable difference for complex models but was less impactful for simpler ones.\n",
        "\n",
        "4. Why Fast Prediction Times Matter\n",
        "Inference time, or how quickly models can make predictions, is important in practical use.\n",
        "\n",
        "Quick Models for Real-Time Use: Models like Logistic Regression and Decision Trees make predictions quickly, which is ideal for real-time applications like fraud detection, where immediate responses are needed.\n",
        "\n",
        "Balancing Training and Prediction Times: Some models, like Random Forests, take longer to train but are fast at making predictions. This can be useful when both accuracy and speed are required.\n",
        "\n",
        "Overall, faster prediction times are important for tasks that need quick responses.\n",
        "\n",
        "5. Handling Imbalanced Data\n",
        "For our imbalanced dataset, accuracy alone doesn’t give a full picture. We also looked at confusion matrix and F1 score to see how well models handled both classes.\n",
        "\n",
        "Confusion Matrix: This shows true positives, false positives, true negatives, and false negatives, especially for the minority class. Models with higher true positives and fewer false negatives are better at identifying cases from the minority class.\n",
        "\n",
        "For example, [Model E] showed high true positive rates with fewer false negatives, making it more reliable at catching minority class instances.\n",
        "\n",
        "F1 Score: F1 score balances precision and recall, which is helpful for imbalanced data. A high F1 score suggests that a model can identify true positives while keeping false positives low. [Model F] had the highest F1 score, making it effective at handling imbalanced data.\n",
        "\n",
        "Confusion Matrix and F1 Score Comparison\n",
        "Below are the confusion matrices and F1 scores for each model, showing how well each handled the minority class.\n",
        "\n",
        "These metrics show why it’s important to use additional evaluations beyond accuracy for imbalanced data. Looking at F1 scores and confusion matrices helps ensure models perform well for both classes."
      ],
      "metadata": {
        "id": "quDIYb9njIqG"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}